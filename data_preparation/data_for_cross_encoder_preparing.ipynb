{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4OM6Ks1u-aVu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E43-aWBd78Kw",
        "outputId": "ccf0677b-1a67-43f4-8d7a-e2f1d18db1ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Первые строки датасета Доктора Хауса:\n",
            "    name  season                                      previous_line  \\\n",
            "0  house       1  Fair enough. I dont like healthy Patients. The...   \n",
            "1  house       1  Shouldnt we be speaking to the Patient before ...   \n",
            "2  house       1                                           No, but!   \n",
            "3  house       1      Isnt treating Patients why we became doctors?   \n",
            "4  house       1                                           Mad cow?   \n",
            "\n",
            "  previous_speaker                                       context_long  \\\n",
            "0           wilson  See that? They all assume Im a Patient because...   \n",
            "1          foreman  And shes not responding to radiation treatment...   \n",
            "2          foreman  Come on! Why leave all the fun for the coroner...   \n",
            "3          foreman  Shouldnt we be speaking to the Patient before ...   \n",
            "4            chase  First year of medical school if you hear hoof ...   \n",
            "\n",
            "   is_greeting  is_question  is_negation  is_exclamation  is_sarcasm  \\\n",
            "0        False        False        False           False       False   \n",
            "1        False         True        False           False       False   \n",
            "2        False        False        False           False       False   \n",
            "3        False        False         True           False       False   \n",
            "4        False        False        False           False       False   \n",
            "\n",
            "   is_answer                                          context_1  \\\n",
            "0      False  You see where the administration might have a ...   \n",
            "1       True                     Its a lesion. Is she a doctor?   \n",
            "2      False  Shouldnt we be speaking to the Patient before ...   \n",
            "3       True  No, but! No, treating illnesses is why we beca...   \n",
            "4       True  Aneurysm, stroke, or some other ischemic syndr...   \n",
            "\n",
            "                                           context_2  \\\n",
            "0  So put on a white coat like the rest of us. Yo...   \n",
            "1  And shes not responding to radiation treatment...   \n",
            "2  Its a lesion. Shouldnt we be speaking to the P...   \n",
            "3  Shouldnt we be speaking to the Patient before ...   \n",
            "4  First year of medical school if you hear hoof ...   \n",
            "\n",
            "                                           context_3  \\\n",
            "0  29 year old female, first seizure one month ag...   \n",
            "1  Other side. No environmental factors. And shes...   \n",
            "2  And shes not responding to radiation treatment...   \n",
            "3  Its a lesion. Shouldnt we be speaking to the P...   \n",
            "4  So youre trying to eliminate the huManity from...   \n",
            "\n",
            "                                           context_4  \\\n",
            "0                                                NaN   \n",
            "1  No family history. Other side. No environmenta...   \n",
            "2  Other side. No environmental factors. And shes...   \n",
            "3  And shes not responding to radiation treatment...   \n",
            "4  Isnt treating Patients why we became doctors? ...   \n",
            "\n",
            "                                           context_5  is_self_question  \\\n",
            "0                                                NaN             False   \n",
            "1  Protein markers for the three most prevalent b...             False   \n",
            "2  No family history. Other side. No environmenta...             False   \n",
            "3  Other side. No environmental factors. And shes...             False   \n",
            "4  No, but! Isnt treating Patients why we became ...             False   \n",
            "\n",
            "                                                line  \n",
            "0          The one who cant talk, I liked that part.  \n",
            "1                                    Is she a doctor  \n",
            "2                                    Everybody lies.  \n",
            "3  No, treating illnesses is why we became doctor...  \n",
            "4                                         Mad zebra.  \n",
            "Первые строки датасета антагонистов:\n",
            "     character                                               line\n",
            "0  Ross Geller         Just a sandwich. Turkey, a little mustard…\n",
            "1  Ross Geller                                                NaN\n",
            "2  Ross Geller                                    Oh, absolutely!\n",
            "3  Ross Geller  please dont cry because of me pheebs I dont kn...\n",
            "4  Ross Geller                                              Cmon.\n"
          ]
        }
      ],
      "source": [
        "# Пути к данным\n",
        "house_path = \"/content/data/house_dataset.csv\"\n",
        "antagonists_path = \"/content/data/antagonists_dataset.csv\"\n",
        "\n",
        "# Загрузка данных\n",
        "house_df = pd.read_csv(house_path)\n",
        "antagonists_df = pd.read_csv(antagonists_path)\n",
        "\n",
        "# Вывод первых строк для проверки\n",
        "print(\"Первые строки датасета Доктора Хауса:\")\n",
        "print(house_df.head())\n",
        "\n",
        "print(\"Первые строки датасета антагонистов:\")\n",
        "print(antagonists_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQid1Z3-_ooG"
      },
      "source": [
        "**Отличия очистки данных для кросс-энкодера**\n",
        "\n",
        "Очистка данных для кросс-энкодера отличается от стандартной предобработки, так как модель оценивает соответствие пары \"контекст + вопрос → ответ\". Основные различия:\n",
        "\n",
        "1. Контекст должен оставаться информативным. Не удаляем вопросы из контекста (previous_line, context_1, context_2 и т. д.), потому что они важны для понимания диалога.\n",
        "Вопрос из контекста помогает модели различать правильные и неправильные ответы.\n",
        "\n",
        "2.  Негативные примеры (антагонисты) должны быть осмысленными. Оставляем все антагонистические реплики, но удаляем вопросы, чтобы не запутывать модель. Негативные примеры должны быть похожи на реальные ответы, но не соответствовать контексту.\n",
        "3. Ограничение длины контекста. BERT-кросс-энкодер имеет ограничение 512 токенов, поэтому: оставляем только 2-3 предыдущие реплики перед вопросом (context_1 – context_3);\n",
        "ограничиваем контекст до 100 слов (больше, чем для биэнкодера), чтобы не перегружать модель. Ограничиваем ответы до 40 слов, чтобы избежать обрезания важных частей.\n",
        "4. Фильтрация коротких реплик. Для Доктора Хауса фильтруем реплики короче 3 слов. Для антагонистов оставляем вообще все реплики, кроме вопросов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SR1HEq28z0w",
        "outputId": "a6070bad-99e8-431b-a302-0caddeb35c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Количество реплик после очистки: 7417 (Доктор Хаус), 76386 (Антагонисты).\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# --- ФУНКЦИИ ОЧИСТКИ ---\n",
        "def clean_text(text):\n",
        "    \"\"\"Очистка текста от проблемных символов, лишних пробелов и мусора.\"\"\"\n",
        "    if not isinstance(text, str) or len(text) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    text = text.replace(\"�\", \"\")  # Удаление неизвестных символов\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Убираем лишние пробелы\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)  # Удаляем любые не-ASCII символы (если есть)\n",
        "\n",
        "    return text\n",
        "\n",
        "def additional_cleaning(text):\n",
        "    \"\"\"Дополнительная очистка с учетом чисел, процентов, дробей и случайных заглавных букв.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s.,!?'$/%-]\", \"\", text)  # Разрешенные символы\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Удаление лишних пробелов\n",
        "\n",
        "    return text\n",
        "\n",
        "def apply_text_cleaning(df, text_columns):\n",
        "    \"\"\"Очистка всех указанных текстовых колонок.\"\"\"\n",
        "    for col in text_columns:\n",
        "        df[col] = df[col].astype(str).apply(clean_text).apply(additional_cleaning)\n",
        "    return df\n",
        "\n",
        "# --- ФИЛЬТРАЦИЯ ДАННЫХ ---\n",
        "def count_words(text):\n",
        "    \"\"\"Подсчет количества слов в тексте.\"\"\"\n",
        "    return len(text.split()) if isinstance(text, str) else 0\n",
        "\n",
        "def filter_dataframe(df, min_word_count, remove_questions=True, text_column=\"line\"):\n",
        "    \"\"\"Фильтрация реплик: удаление коротких, дубликатов, вопросов и лишних символов.\"\"\"\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Удаление пустых значений\n",
        "    df = df.dropna(subset=[text_column]).reset_index(drop=True)\n",
        "\n",
        "    # Удаление слишком коротких реплик (<10 символов)\n",
        "    df = df[df[text_column].str.len() >= 10].reset_index(drop=True)\n",
        "\n",
        "    # Удаление вопросов, если это колонка с ответами (`line`)\n",
        "    if remove_questions:\n",
        "        df = df[~df[text_column].str.endswith(\"?\")].reset_index(drop=True)\n",
        "\n",
        "    # Удаление дубликатов\n",
        "    df = df.drop_duplicates(subset=[text_column]).reset_index(drop=True)\n",
        "\n",
        "    # Фильтрация реплик по количеству слов\n",
        "    df[\"word_count\"] = df[text_column].apply(count_words)\n",
        "    df = df[df[\"word_count\"] >= min_word_count].drop(columns=[\"word_count\"]).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- СОКРАЩЕНИЕ КОНТЕКСТА ---\n",
        "def get_shortened_context(row, max_replies=3):\n",
        "    \"\"\"Формирует контекст, ограниченный 2-3 репликами перед вопросом.\"\"\"\n",
        "    context_parts = []\n",
        "\n",
        "    for i in range(1, max_replies + 1):  # Оставляем context_1, context_2, context_3\n",
        "        if pd.notna(row[f\"context_{i}\"]):\n",
        "            context_parts.append(row[f\"context_{i}\"])\n",
        "\n",
        "    return \" [SEP] \".join(context_parts)\n",
        "\n",
        "def truncate_text(text, max_words):\n",
        "    \"\"\"Обрезает текст до указанного количества слов.\"\"\"\n",
        "    words = text.split()\n",
        "    return \" \".join(words[:max_words]) if len(words) > max_words else text\n",
        "\n",
        "# --- ОЧИСТКА И ФИЛЬТРАЦИЯ ---\n",
        "# Очистка текстовых колонок\n",
        "text_columns_house = [\"line\", \"previous_line\", \"context_1\", \"context_2\", \"context_3\", \"context_4\", \"context_5\", \"context_long\"]\n",
        "text_columns_antagonists = [\"line\"]\n",
        "\n",
        "house_df = apply_text_cleaning(house_df, text_columns_house)\n",
        "antagonists_df = apply_text_cleaning(antagonists_df, text_columns_antagonists)\n",
        "\n",
        "# Фильтрация данных\n",
        "house_df = filter_dataframe(house_df, min_word_count=3, remove_questions=True)\n",
        "antagonists_df = filter_dataframe(antagonists_df, min_word_count=1, remove_questions=True)\n",
        "\n",
        "# Генерация укороченного контекста\n",
        "house_df[\"short_context\"] = house_df.apply(get_shortened_context, axis=1)\n",
        "\n",
        "# Обрезаем слишком длинные реплики\n",
        "house_df[\"line\"] = house_df[\"line\"].apply(lambda x: truncate_text(x, 40))\n",
        "house_df[\"short_context\"] = house_df[\"short_context\"].apply(lambda x: truncate_text(x, 100))\n",
        "\n",
        "# Итоговая статистика\n",
        "print(f\"\\nКоличество реплик после очистки: {len(house_df)} (Доктор Хаус), {len(antagonists_df)} (Антагонисты).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZpT0LhC_nwf",
        "outputId": "7940ff5e-5803-48a9-ec94-181b00c421a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Количество вопросов в предыдущих репликах:\n",
            "previous_line: 5802 (78.23%)\n",
            "context_1: 4352 (58.68%)\n",
            "context_2: 5324 (71.78%)\n",
            "context_3: 5976 (80.57%)\n",
            "context_4: 6423 (86.60%)\n",
            "context_5: 6709 (90.45%)\n",
            "\n",
            "Примеры вопросов в предыдущих репликах:\n",
            "\n",
            "previous_line:\n",
            "- It doesnt matter. I clearly didnt lead him along or anything like that, which proves Im not a tease. So why is your girlfriend mad at you?\n",
            "- So you just show up Every time hes at physio?\n",
            "- Youre cutting him open?\n",
            "- And why are you carrying a vial of it around with you?\n",
            "- Youre talking about me?\n",
            "\n",
            "context_1:\n",
            "- Hes my ex, I Youve got an opinion, too?\n",
            "- Im 4 1. Thats 1.5 canes in metric. Compared to you Im sure he was huge. Did he have a fetish or did he just fall in love with your longlegged soul?\n",
            "- And you beliEve him? I understand youre a big fan. Ill have my guy send over a signed glossy.\n",
            "- Thats not a speech. A few things I forgot to mention. Ed Vogler is a brilliant businessMan. A brilliant Judge of people, and a Man who has nEver lost a fight. You know how I know the new ACE inhibitor is good? Because the old one was good. The new one is really the same, its just more expensive. A lot more expensive. See, thats another example of Eds brilliance. WhenEver one of his drugs is about to lose its patent he has his boys and girls alter it just a tiny bit and patent it all over again. Making not just a pointless new pill, but millions and millions of dollars. Which is good for Everbody, right? The Patients, pish. Who cares, theyre just so damn sick! God obviously nEver liked them anyway. All the healthy people in the room, lets have a big round of applause for Ed Vogler! I threw in a joke.\n",
            "- Youre only saying that to make me go. Well always have Fresno. Im no good at being noble but it doesnt take much to see that the pRoblems of two little people dont amount to a hill of beans in this crazy world. Someday youll understand that. Now, now, heres looking at you Damn. Was there an earthquake when you were in Fresno?\n",
            "\n",
            "context_2:\n",
            "- I saw you leaving last Tuesday practically tripped over two guys on your way out. But you had no pRoblem opening doors. Its called Akinetopsia. You cant see things when they move. And since you havent been hit by a bus, I assume its intermittent. PRobably accompanied by seizures, which made me think that I can set one off by flashing a... He didnt report being an alcoholic either. Drinking equals falling down equals trauma equals... Its not his EEG. Its his fathers. When it comes to cortical seizures, like father, like son.\n",
            "- Because this is a bad idea, House. Spoiled grandParkldren. And I lose my temper when someone suggests that a handful of poppsych catch phrases can control our most seminal, intractable, and natural reaction to a dead Parkld. Am I right, Emory?\n",
            "- Shes crashing again. Where are you? What? Im shocked! Oh, no, wait a minute thats you.\n",
            "- Thoracic tumor is a better fit. Erodes into her airway and esophagus Stop what? Its not an opinion. Its a smoke screen. Toss out a lame idea, instead of agreeing with Foremans better idea because youre worried thatll confirm that hes boldly gone where no Man has gone before.\n",
            "- Ive always thought shes looked sexiest when she was pregnant. Something bout knowing shes a mother makes me want to I did. You gotta talk to her, I couldnt bear it if something happened to her You know where to find us. Buildings not going anywhere.\n",
            "\n",
            "context_3:\n",
            "- Are you arguing that hes a good role model? Youre stealing this guys oxygen? You passed on all your cases, reassigned them to other doctors. Yeah, well, I guess youll have to tell the parole board something else. Maybe that I was in the O.R. the entire day the ceiling collapsed, so I couldnt have caused the plumbing pRoblem.\n",
            "- That youre an ass? Yeah. You overheard me talking to my dead girlfriend and thought to yourself, what kind of fun can I have with this? Patient Lauren Maybaum, 27, presented two days ago with sEvere abdominal pain. Yesterday you said you werent ready. Feeling much better. Thank you for not asking.\n",
            "- Oh I didnt know you were seeing a Patient. What you usually do hiding from Cuddy. Ah no, I, I.... Im not going.\n",
            "- I am warning you, do not Hes blind. We need something to bag the sample. Apparently I was optimistic about the 36 hours. Intractable, unbearable pain is up next. Sure you dont want to reconsider that whole autopsy thing?\n",
            "- She wants you. You only came back because I freaked out. Right? Thank you. Im sorry I needed you. See? This is why I lied about the phone. Your BP is spiking, so youre bleeding faster from your leg wound. Im gonna have less time to save it. Hang up.\n",
            "\n",
            "context_4:\n",
            "- And then there was none. We still have the pRoblem of explaining how a white Parkck from Jersey whos nEver traveled south of D.C. has African sleeping sickness. I made it clear. If this guys lying about sleeping around, he knows hes murdering his wife. Does seem unlikely! Go away. What are you doing? Im Dr. House. Your wife has huMan African Trypanosomiasis. Sleeping sickness.\n",
            "- Whatd you do to him? Youll tell me. Anything you say is attorneyclient. So you can get advice about the bad, bad thing you did, knowing Ill be tortured because I cant tell a soul. Are you completely out of your mind? Shes dying on her own, why would I volunteer to be her executioner? Id just be inviting a lawsuit from the brother, no matter what! Its tax free.\n",
            "- Liver function has improved 30%! buys us a few extra days before hell need a transplant. Interesting question is, why is it regenerating? We need to run a scratch test to find out what set it off. Hes allergic to a person? I take it youre in the latter Category.\n",
            "- That... was talking. Yoga girl walked out of here 2 hours ago, you fixed her. He had brain cancer they removed it 8 years ago. His conditions been the same Ever since. Whatd he say? Sounds good. Call me when youre done.\n",
            "- Its too late! We killed him. What are you doing? You cant do this! You cant do this! Hes stable on the ventilator. Oxygenating well. Everyone knows whats wrong with me. Whats wrong with him is much more interesting.\n",
            "\n",
            "context_5:\n",
            "- He cures thousands of people Every year, you cure, what? 30? Oh, I see! So you hate him because the lives he saves arent as good as the lives you save. The top of my heads killing me. Ow! That is not the top of my head! It was my mothers. Shes dead. Oh. Poor cat. Youre allergic. We can control it with antihistamine, one pill a day.\n",
            "- Cuddy has a mass on her kidney? And then you pointed out how sweet that was. While hes doing that, can I at least search his House for drugs? Have I Ever said no to that question? Pieeating contest. Three I hate the smell of death.\n",
            "- Im a doctor, shes used to being alone. I dont want to talk about it. You did good with the nun. Congratulations. This is a good hospital? Its not the alcohol, its gotta be something else. Id be happy to refer you the case, Dr. House. You seem so interested. Own my own stethoscope. Did I ask you how old she was? I forget.\n",
            "- I was wrong. It worked!?! House, it worked. That would pRobably be inappropriate, cause Im your doctor and Everything. Good luck with Everything. It! Uh, blended? Single malt? Any preference? Noooooooooooooo! What do you do when you win?\n",
            "- The baby wont feel a thing. It must be easier to hear that you might die than your baby might die. But if theres anyone I would trust to save my baby, it would be Dr. House. Chase isnt the one whos gonna get hurt here. My social life is my social life. She told me to end it. Is that what you want? Trying to avoid altitude sickness. Couldnt score a direct flight to Cambodia so I decided to scale Argentinas infamous Mount Aconcagua\n"
          ]
        }
      ],
      "source": [
        " import re\n",
        "\n",
        "def is_question(text):\n",
        "    \"\"\"Определяет, содержит ли реплика вопрос\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return False\n",
        "\n",
        "    # Проверка, заканчивается ли текст на вопросительный знак\n",
        "    if text.strip().endswith(\"?\"):\n",
        "        return True\n",
        "\n",
        "    # Поиск вопросительных слов в тексте\n",
        "    question_words = {\"what\", \"why\", \"how\", \"who\", \"when\", \"where\", \"which\",\n",
        "                      \"что\", \"почему\", \"как\", \"кто\", \"когда\", \"где\", \"какой\", \"зачем\"}\n",
        "\n",
        "    words = set(text.lower().split())\n",
        "    if question_words.intersection(words):\n",
        "        return True\n",
        "\n",
        "    # Проверка наличия вопросительного знака внутри реплики\n",
        "    if \"?\" in text:\n",
        "        return True\n",
        "\n",
        "    # Проверка восклицательных предложений, содержащих вопросительные слова\n",
        "    if text.endswith(\"!\") and question_words.intersection(words):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def analyze_questions(df, context_columns):\n",
        "    \"\"\"Подсчитывает количество вопросов в указанных колонках\"\"\"\n",
        "    question_stats = {col: df[col].apply(is_question).sum() for col in context_columns}\n",
        "    return question_stats\n",
        "\n",
        "# Определение колонок для анализа\n",
        "context_columns = [\"previous_line\", \"context_1\", \"context_2\", \"context_3\", \"context_4\", \"context_5\"]\n",
        "\n",
        "# Анализ вопросов в контексте\n",
        "question_stats = analyze_questions(house_df, context_columns)\n",
        "\n",
        "# Вывод статистики по вопросам\n",
        "print(\"\\nКоличество вопросов в предыдущих репликах:\")\n",
        "for col, count in question_stats.items():\n",
        "    total = len(house_df)\n",
        "    print(f\"{col}: {count} ({count / total * 100:.2f}%)\")\n",
        "\n",
        "# Выборка примеров реплик с вопросами\n",
        "print(\"\\nПримеры вопросов в предыдущих репликах:\")\n",
        "for col in context_columns:\n",
        "    sample_questions = house_df[house_df[col].apply(is_question)][col].dropna().sample(5, random_state=42).tolist()\n",
        "    print(f\"\\n{col}:\")\n",
        "    for q in sample_questions:\n",
        "        print(f\"- {q}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dpM8OPjB56D",
        "outputId": "3fbc2283-d538-4c33-983b-f23ec185d018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Распределение количества вопросов в предыдущей реплике:\n",
            "0 вопросов: 1615 реплик (21.77%)\n",
            "1 вопросов: 5084 реплик (68.55%)\n",
            "2 вопросов: 617 реплик (8.32%)\n",
            "3 вопросов: 83 реплик (1.12%)\n",
            "4 вопросов: 13 реплик (0.18%)\n",
            "5 вопросов: 3 реплик (0.04%)\n",
            "6 вопросов: 1 реплик (0.01%)\n",
            "8 вопросов: 1 реплик (0.01%)\n"
          ]
        }
      ],
      "source": [
        "def count_questions(text):\n",
        "    \"\"\"Определяет количество вопросов в реплике\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "\n",
        "    # Разбиваем текст на предложения\n",
        "    sentences = re.split(r\"(?<=[?.!])\\s+\", text)\n",
        "\n",
        "    # Счетчик вопросов\n",
        "    question_count = sum(1 for s in sentences if is_question(s))\n",
        "\n",
        "    return question_count\n",
        "\n",
        "# Добавляем колонку с количеством вопросов в предыдущей реплике\n",
        "house_df[\"question_count_previous\"] = house_df[\"previous_line\"].apply(count_questions)\n",
        "\n",
        "# Анализ распределения\n",
        "question_distribution = house_df[\"question_count_previous\"].value_counts().sort_index()\n",
        "\n",
        "# Вывод распределения количества вопросов в предыдущей реплике\n",
        "print(\"\\nРаспределение количества вопросов в предыдущей реплике:\")\n",
        "for num_questions, count in question_distribution.items():\n",
        "    print(f\"{num_questions} вопросов: {count} реплик ({count / len(house_df) * 100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tS8zEwLCR6C"
      },
      "source": [
        "Выводы из распределения количества вопросов в предыдущей реплике\n",
        "1. Большинство реплик содержит ровно один вопрос (68.55%), то есть основной контекст перед ответом — это одиночный вопрос.\n",
        "Вероятно, в диалоге чаще всего звучит один явный вопрос, на который отвечает Доктор Хаус. Включение такого контекста в кросс-энкодер важно, потому что модель должна понимать, что основной сигнал для ответа — это вопрос.\n",
        "2. 21.77% реплик вообще не содержат вопросов. Иногда ответы следуют после утверждений или размышлений, а не после вопросов.\n",
        "Нужно не исключать такие реплики, но учитывать, что иногда контекст — не явный вопрос. Для кросс-энкодера это означает, что иногда предыдущая реплика — это не вопрос, а комментарий.\n",
        "3. 8.32% реплик содержат два вопроса. В таких случаях возможно два варианта: а) Вопросы идут подряд, например: \"Ты серьезно? Почему?\" б) Вопрос встроен в реплику: \"Ты уверен в этом? Я просто не понимаю, как это может работать.\" Для кросс-энкодера это означает, что иногда нужно учитывать оба вопроса, а не только последний.\n",
        "4. Редкие случаи с 3+ вопросами (1.35%) Такие реплики встречаются редко, поэтому не стоит делать на них акцент.\n",
        "Однако, если в реплике 3+ вопросов, скорее всего, важен последний.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75cQs7kFDpUx",
        "outputId": "8b36b523-a162-4db6-ef72-aa2ad5829313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Распределение длины предыдущих реплик и контекста:\n",
            "       previous_line_length  context_1_length  context_2_length  \\\n",
            "count            7417.00000       7417.000000       7417.000000   \n",
            "mean               10.14251         28.756775         39.188756   \n",
            "std                 9.81743         20.133525         22.890445   \n",
            "min                 1.00000          3.000000          5.000000   \n",
            "25%                 4.00000         15.000000         24.000000   \n",
            "50%                 7.00000         23.000000         34.000000   \n",
            "75%                13.00000         36.000000         49.000000   \n",
            "90%                21.00000         53.000000         67.000000   \n",
            "95%                28.00000         65.000000         82.000000   \n",
            "max               152.00000        192.000000        202.000000   \n",
            "\n",
            "       context_3_length  context_4_length  context_5_length  \\\n",
            "count       7417.000000       7417.000000       7417.000000   \n",
            "mean          49.940812         60.336524         70.927734   \n",
            "std           25.582663         27.841888         30.007378   \n",
            "min            7.000000          1.000000          1.000000   \n",
            "25%           32.000000         41.000000         50.000000   \n",
            "50%           45.000000         55.000000         66.000000   \n",
            "75%           62.000000         74.000000         86.000000   \n",
            "90%           82.000000         96.000000        108.000000   \n",
            "95%           98.000000        111.000000        126.000000   \n",
            "max          218.000000        236.000000        276.000000   \n",
            "\n",
            "       context_long_length  \n",
            "count          7417.000000  \n",
            "mean             77.413779  \n",
            "std              33.291627  \n",
            "min              12.000000  \n",
            "25%              54.000000  \n",
            "50%              72.000000  \n",
            "75%              94.000000  \n",
            "90%             121.000000  \n",
            "95%             139.000000  \n",
            "max             262.000000  \n"
          ]
        }
      ],
      "source": [
        "def get_text_length(text):\n",
        "    \"\"\"Возвращает количество слов в тексте\"\"\"\n",
        "    return len(text.split()) if isinstance(text, str) else 0\n",
        "\n",
        "# Добавление колонок с длиной текста\n",
        "length_columns = [\"previous_line\", \"context_1\", \"context_2\", \"context_3\", \"context_4\", \"context_5\", \"context_long\"]\n",
        "for col in length_columns:\n",
        "    house_df[f\"{col}_length\"] = house_df[col].apply(get_text_length)\n",
        "\n",
        "# Анализ распределения длины текста\n",
        "length_stats = house_df[[f\"{col}_length\" for col in length_columns]].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])\n",
        "\n",
        "# Вывод статистики по длине реплик\n",
        "print(\"\\nРаспределение длины предыдущих реплик и контекста:\")\n",
        "print(length_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVw1jyuNEOHZ"
      },
      "source": [
        "1. Предыдущая реплика — в среднем 10 слов, но иногда до 152 слов. В 50% случаев содержит 7 слов или меньше. В 90% случаев не превышает 21 слово.\n",
        "Вывод: previous_line можно включать целиком в контекст, но если он длиннее 25 слов, стоит обрезать.\n",
        "2. context_1 — в среднем 28 слов, но может достигать 192 слов\n",
        "Вывод: context_1 можно включать, но если он длиннее 50 слов, обрезать.\n",
        "3. context_2 и далее — контекст быстро растет. context_2 в среднем 39 слов (до 202 слов). context_3 в среднем 49 слов (до 218 слов). context_4, context_5 уже 60+ слов.\n",
        "context_long (все предыдущие реплики) в среднем 77 слов (до 262 слов).\n",
        "Вывод: context_3+ включать только при необходимости, иначе он будет слишком длинным.\n",
        "\n",
        "**Решение по формированию контекста.**\n",
        "- Берем previous_line полностью, если ≤25 слов, иначе обрезаем.\n",
        "- Берем context_1, если в нем есть вопрос, иначе context_2.\n",
        "- Обрезаем context_1 и context_2, если они >50 слов.\n",
        "context_3+ включать только если суммарная длина < 100 слов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8KvI_8WFDLH",
        "outputId": "5acdb468-3006-4ac0-f508-4a037f9be07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры сформированных контекстов:\n",
            "\n",
            "Контекст 1:\n",
            "Fair enough. I dont like healthy Patients. The 29 year old female! [SEP] You see where the administration might have a pRoblem with that attitude. The one who cant talk, I liked that part. [SEP] So put on a white coat like the rest of us. You see where the administration might have a pRoblem with that attitude. The one who cant talk, I liked that part.\n",
            "\n",
            "Контекст 2:\n",
            "Shouldnt we be speaking to the Patient before we start diagnosing? [SEP] Its a lesion. Is she a doctor? [SEP] And shes not responding to radiation treatment. Its a lesion. Is she a doctor?\n",
            "\n",
            "Контекст 3:\n",
            "Isnt treating Patients why we became doctors? [SEP] No, but! No, treating illnesses is why we became doctors, treating Patients is what makes most doctors miserable. [SEP] Shouldnt we be speaking to the Patient before we start diagnosing? No, but! No, treating illnesses is why we became doctors, treating Patients is what makes most doctors miserable.\n",
            "\n",
            "Контекст 4:\n",
            "Wernickies encephalopathy? [SEP] Mad cow? No, blood thiamine lEvel was normal. [SEP] Aneurysm, stroke, or some other ischemic syndrome. Mad cow? No, blood thiamine lEvel was normal.\n",
            "\n",
            "Контекст 5:\n",
            "You think we have nothing to talk about? [SEP] Wernickies encephalopathy? Lab in Trenton could have screwed up the blood test. I assume its a corollary if people lie, that people screw up. I was expecting you in my office 20 minutes ago. No, just that I cant think of anything that Id be interested in. [SEP] Lab in Trenton could have screwed up the blood test. I assume its a corollary if people lie, that people screw up. I was expecting you in my office 20 minutes ago. No, just that I cant think of anything that Id be interested in.\n"
          ]
        }
      ],
      "source": [
        "def truncate_text(text, max_words):\n",
        "    \"\"\"Обрезает текст до указанного количества слов.\"\"\"\n",
        "    words = text.split()\n",
        "    return \" \".join(words[:max_words]) if len(words) > max_words else text\n",
        "\n",
        "def select_best_context(row):\n",
        "    \"\"\"Формирует контекст на основе длины и наличия вопросов\"\"\"\n",
        "\n",
        "    # Обрезка предыдущей реплики, если слишком длинная\n",
        "    previous = truncate_text(row[\"previous_line\"], 25)\n",
        "\n",
        "    # Поиск первого контекста с вопросом\n",
        "    selected_context = []\n",
        "    for i in range(1, 6):  # context_1 - context_5\n",
        "        if is_question(row[f\"context_{i}\"]):  # Если есть вопрос, добавляем\n",
        "            selected_context.append(truncate_text(row[f\"context_{i}\"], 50))\n",
        "            break\n",
        "\n",
        "    # Если вопросов в контексте нет, берем context_1\n",
        "    if not selected_context and pd.notna(row[\"context_1\"]):\n",
        "        selected_context.append(truncate_text(row[\"context_1\"], 50))\n",
        "\n",
        "    # Если контекст не превышает 100 слов, добавляем context_2\n",
        "    total_length = sum(len(c.split()) for c in selected_context)\n",
        "    if total_length < 100 and pd.notna(row[\"context_2\"]):\n",
        "        selected_context.append(truncate_text(row[\"context_2\"], 50))\n",
        "\n",
        "    # Финальный контекст\n",
        "    final_context = \" [SEP] \".join([previous] + selected_context)\n",
        "    return final_context\n",
        "\n",
        "# Генерация финального контекста\n",
        "house_df[\"final_context\"] = house_df.apply(select_best_context, axis=1)\n",
        "\n",
        "# Проверка примеров\n",
        "print(\"\\nПримеры сформированных контекстов:\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nКонтекст {i+1}:\")\n",
        "    print(house_df[\"final_context\"].iloc[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ33WRbKFb38",
        "outputId": "54bb89b9-27e4-4dc3-d287-896514645582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Статистика по длине сформированного контекста:\n",
            "count    7417.000000\n",
            "mean       76.700822\n",
            "std        26.311406\n",
            "min        13.000000\n",
            "25%        55.000000\n",
            "50%        77.000000\n",
            "75%       101.000000\n",
            "90%       110.000000\n",
            "95%       115.000000\n",
            "max       127.000000\n",
            "Name: final_context_length, dtype: float64\n",
            "\n",
            "Статистика по количеству вопросов в контексте:\n",
            "count    7417.000000\n",
            "mean        3.135095\n",
            "std         1.674280\n",
            "min         0.000000\n",
            "25%         2.000000\n",
            "50%         3.000000\n",
            "75%         4.000000\n",
            "max        12.000000\n",
            "Name: final_context_question_count, dtype: float64\n",
            "\n",
            "Частота использования разных уровней контекста:\n",
            "context_1: 7417 раз использован (100.00%)\n",
            "context_2: 7417 раз использован (100.00%)\n",
            "context_3: 7417 раз использован (100.00%)\n",
            "context_4: 7417 раз использован (100.00%)\n",
            "context_5: 7417 раз использован (100.00%)\n",
            "\n",
            "Примеры длинных контекстов:\n",
            "\n",
            "Контекст 1:\n",
            "What time is it? [SEP] I go to church mainly to keep my wife happy, but! I dont know, Ive nEver actually thought that God could reach out and affect my life. But getting a brilliant doctor in the bed next to me? I think Molly was right. I think you were sent here to [SEP] I go to church mainly to keep my wife happy, but! I dont know, Ive nEver actually thought that God could reach out and affect my life. But getting a brilliant doctor in the bed next to me? I think Molly was right. I think you were sent here to\n",
            "\n",
            "Контекст 2:\n",
            "No, she thinks if she looks different, shell be more attractive, which, I have to say! [SEP] Well. Thats what breasts look like. Well, if a tree pretends to fall in a forest! House, come on, theyre breasts. Theyre a birthday present, not a philosophical treatise. Not to her husband. Cosmetic surgery is so that Everyone else will look at us differently. Same reason youre wearing that [SEP] Well. Thats what breasts look like. Well, if a tree pretends to fall in a forest! House, come on, theyre breasts. Theyre a birthday present, not a philosophical treatise. Not to her husband. Cosmetic surgery is so that Everyone else will look at us differently. Same reason youre wearing that\n",
            "\n",
            "Контекст 3:\n",
            "Chase? Can you get me out of Ohh! Hello? [SEP] Patients stroking out. The good news is, its isolated ophthalmoplegia, which means you didnt have a stroke. Bad news is, Taub was right. Youre still sick. I said right, not competent. Bursts your CTE bubble. Youre going to need a whole new theory to keep Uncle Jesse and his Olsen [SEP] Thats a density. I know you think Im convincing myself, but this brain has been hammered for years, meaning evidence may be subtle, but its there. Patients stroking out. The good news is, its isolated ophthalmoplegia, which means you didnt have a stroke. Bad news is, Taub was right. Youre\n",
            "\n",
            "Контекст 4:\n",
            "How did you find that without a stethoscope? I didnt Even know you could do this! [SEP] The ol swap trick? You dont think that will piss off your cellie Even more when he figures its a different bug? Well, Ill try, but they spray so much pesticide Im surprised were alive. It aint. The Confederates be giving themselves spankin new tats. They putting a claim on [SEP] It aint. The Confederates be giving themselves spankin new tats. They putting a claim on all of my Vicodin. Now, I love you and all, my brother, but not as much as I love my gang of crazy, roidedout Nazi bitches. Yeah! Start the blood thinners.\n",
            "\n",
            "Контекст 5:\n",
            "What, are you trying to scare me now? [SEP] I did not have a relative It means youre not just sick in the head. The pRoblem is, the rest of you appears well, so Ive got to make you seem as sick as youre supposed to be by injecting you with a drug that simulates the symptoms that you [SEP] I didnt fake my way I did not have a relative It means youre not just sick in the head. The pRoblem is, the rest of you appears well, so Ive got to make you seem as sick as youre supposed to be by injecting you with a drug that\n"
          ]
        }
      ],
      "source": [
        "def count_questions_in_text(text):\n",
        "    \"\"\"Подсчитывает количество вопросов в тексте\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "    sentences = re.split(r\"(?<=[?.!])\\s+\", text)\n",
        "    return sum(1 for s in sentences if is_question(s))\n",
        "\n",
        "# Добавление статистики\n",
        "house_df[\"final_context_length\"] = house_df[\"final_context\"].apply(get_text_length)\n",
        "house_df[\"final_context_question_count\"] = house_df[\"final_context\"].apply(count_questions_in_text)\n",
        "\n",
        "# Анализ, какие контексты чаще всего используются\n",
        "context_usage = {\"context_1\": 0, \"context_2\": 0, \"context_3\": 0, \"context_4\": 0, \"context_5\": 0}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    context_usage[f\"context_{i}\"] = house_df[f\"context_{i}\"].notna().sum()\n",
        "\n",
        "# Вывод статистики по длине контекста\n",
        "length_stats = house_df[\"final_context_length\"].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])\n",
        "question_stats = house_df[\"final_context_question_count\"].describe()\n",
        "\n",
        "print(\"\\nСтатистика по длине сформированного контекста:\")\n",
        "print(length_stats)\n",
        "\n",
        "print(\"\\nСтатистика по количеству вопросов в контексте:\")\n",
        "print(question_stats)\n",
        "\n",
        "print(\"\\nЧастота использования разных уровней контекста:\")\n",
        "for key, value in context_usage.items():\n",
        "    print(f\"{key}: {value} раз использован ({value / len(house_df) * 100:.2f}%)\")\n",
        "\n",
        "# Примеры длинных контекстов\n",
        "print(\"\\nПримеры длинных контекстов:\")\n",
        "long_contexts = house_df[house_df[\"final_context_length\"] > 100][\"final_context\"].sample(5, random_state=42).tolist()\n",
        "for i, context in enumerate(long_contexts, 1):\n",
        "    print(f\"\\nКонтекст {i}:\\n{context}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kuRbk-tGPP5",
        "outputId": "560112ac-2823-42fb-e2c3-8cda83514784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Обновленная статистика по длине контекста:\n",
            "count    7417.000000\n",
            "mean       82.502899\n",
            "std        29.096518\n",
            "min        11.000000\n",
            "25%        61.000000\n",
            "50%        79.000000\n",
            "75%       101.000000\n",
            "max       150.000000\n",
            "Name: final_context_length, dtype: float64\n",
            "\n",
            "Обновленная статистика по количеству вопросов:\n",
            "count    7417.000000\n",
            "mean        2.866388\n",
            "std         1.420995\n",
            "min         0.000000\n",
            "25%         2.000000\n",
            "50%         3.000000\n",
            "75%         4.000000\n",
            "max         8.000000\n",
            "Name: final_context_question_count, dtype: float64\n",
            "\n",
            "Примеры длинных контекстов после исправлений:\n",
            "\n",
            "Контекст 1:\n",
            "And he had a full psych evaluation. [SEP] Hes not crazy. [SEP] ER also tested for steroids. [SEP] The negative test at least means steroids is less likely. [SEP] We should discuss other possibilities. [SEP] Could cause the excess hormones that could cause the rage and would elude the ER Steroid test. [SEP] Bilateral venous sampling to find the elevated GnRH. [SEP] MRI to find the pituitary damage. [SEP] Unless, of course, there is no pituitary damage, in which case, our guy got his hands on the good stuff. [SEP] He left a message. [SEP] Said he couldnt get you on your cell or at home. [SEP] I get it. [SEP] Youll pick him up the next time he comes out of prison. [SEP] How do you know about that?\n",
            "\n",
            "Контекст 2:\n",
            "And youre smart, and youre funny but you are bitter. [SEP] And youre lonely, so you treat Everyone around like theyre idiots and you get away with it because of your cane. [SEP] But youre not actually getting away with it. [SEP] Last nurse you made fun of, she pRobably slipped some crap into your coffee. [SEP] Youre kidding me. [SEP] If you have an infection, youd have a fEver. [SEP] Youre chewing nicotine gum which messes with the weather in your mouth so I need to vacation elsewhere. [SEP] Leaving early today. [SEP] Did you Ever get that thing where youre sure youve forgotten something but you cant figure out what? [SEP] Guess it cant be that important. [SEP] And wait til I put the thermometer in. Uh uh, you break it, you bought it.\n",
            "\n",
            "Контекст 3:\n",
            "There are two types of people that hire me. [SEP] No, actually, there are three types of people that hire me, but the third types irrelevant to the point I want to make. [SEP] One type wants to find out that theyre right. [SEP] One type wants to find out that theyre wrong. [SEP] Youre the third type. [SEP] Youre the type that doesnt care if youre right or wrong because theyve hired me to investigate the wrong person. [SEP] You want me to check out Wilson. [SEP] You want to find out if hes You checking me out? [SEP] Have I been paying for that? [SEP] What do I do for a living?\n",
            "\n",
            "Контекст 4:\n",
            "The fact that you still cant admit it just proves my point. [SEP] No, help the guy, whatEver... [SEP] Im just pointing out that its much easier to soothe your guilt by throwing money at a stranger than by making amends to a person you actually care about. [SEP] We were wrong. [SEP] Psychopathy is a symptom. [SEP] I spoke to her sister. [SEP] She wasnt always like this. [SEP] She changed, and right around adolescence. [SEP] We ruled that out. [SEP] No keyserfleischer rings. [SEP] Shes not supposed to be here. [SEP] start her on chelation. [SEP] Her fingernails blue?\n",
            "\n",
            "Контекст 5:\n",
            "What exactly did Cuddy tell you? [SEP] Something like that. [SEP] More that if we told you the truth that youd solved the case based on absolutely no medical proof, that youd think you were God. [SEP] And I was worried your wings would melt. [SEP] Youre not gonna make this easy, are you? [SEP] Unfortunately as much as I admire your spirit, je ne sais rien. [SEP] have your cane. [SEP] the pains returned. [SEP] Im obviously not. [SEP] What is it with you people? [SEP] I dont use the cane, youre shocked. [SEP] I use the cane! [SEP] Were talking about you.\n"
          ]
        }
      ],
      "source": [
        "def select_best_context(row):\n",
        "    \"\"\"Формирует контекст, начиная с самых недавних реплик, удаляя дубли и ограничивая длину\"\"\"\n",
        "\n",
        "    selected_context = []\n",
        "    used_sentences = set()  # Хранит уникальные предложения, чтобы избежать повторов\n",
        "    question_count = 0\n",
        "\n",
        "    # Добавление контекста с конца к началу (context_5 -> context_1)\n",
        "    for i in range(5, 0, -1):  # От context_5 к context_1\n",
        "        ctx = row[f\"context_{i}\"]\n",
        "        if pd.notna(ctx) and ctx not in used_sentences:\n",
        "            # Извлекаются уникальные предложения\n",
        "            sentences = re.split(r\"(?<=[?.!])\\s+\", ctx)\n",
        "            for sentence in sentences:\n",
        "                if len(sentence.split()) > 2 and sentence not in used_sentences:\n",
        "                    used_sentences.add(sentence)\n",
        "                    selected_context.append(sentence)\n",
        "                    if is_question(sentence):\n",
        "                        question_count += 1\n",
        "                    if question_count >= 4:  # Ограничение на 4 вопроса\n",
        "                        break\n",
        "        if question_count >= 4:\n",
        "            break\n",
        "\n",
        "    # Добавление последней реплики (previous_line) в конец\n",
        "    previous = truncate_text(row[\"previous_line\"], 25)\n",
        "    selected_context.append(previous)\n",
        "\n",
        "    # Формирование финального контекста (обрезка с начала, оставляем последние 150 слов)\n",
        "    final_context = \" [SEP] \".join(selected_context)\n",
        "    final_context_words = final_context.split()\n",
        "    if len(final_context_words) > 150:\n",
        "        final_context = \" \".join(final_context_words[-150:])  # Обрезка с начала\n",
        "\n",
        "    return final_context\n",
        "\n",
        "# Применение исправленного кода\n",
        "house_df[\"final_context\"] = house_df.apply(select_best_context, axis=1)\n",
        "\n",
        "# Проверка статистики заново\n",
        "house_df[\"final_context_length\"] = house_df[\"final_context\"].apply(get_text_length)\n",
        "house_df[\"final_context_question_count\"] = house_df[\"final_context\"].apply(count_questions_in_text)\n",
        "\n",
        "print(\"\\nОбновленная статистика по длине контекста:\")\n",
        "print(house_df[\"final_context_length\"].describe())\n",
        "\n",
        "print(\"\\nОбновленная статистика по количеству вопросов:\")\n",
        "print(house_df[\"final_context_question_count\"].describe())\n",
        "\n",
        "# Примеры длинных контекстов после исправлений\n",
        "print(\"\\nПримеры длинных контекстов после исправлений:\")\n",
        "long_contexts = house_df[house_df[\"final_context_length\"] > 100][\"final_context\"].sample(5, random_state=42).tolist()\n",
        "for i, context in enumerate(long_contexts, 1):\n",
        "    print(f\"\\nКонтекст {i}:\\n{context}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvYiO2KtK88g"
      },
      "source": [
        "1. Длина контекста. Средняя длина: 82 слова, максимальная длина: 150 слов, 50% контекстов ≤ 79 слов, что соответствует хорошему балансу.\n",
        "Некоторые контексты по-прежнему длинные (101-150 слов в 25% случаев).\n",
        "\n",
        "2. Количество вопросов. Среднее количество вопросов: 2.86.\n",
        "Максимум: 8 вопросов. В 75% случаев ≤ 4 вопросов, что соответствует нашему лимиту. Иногда остается больше 4 вопросов, что нужно учесть.\n",
        "\n",
        "3. Качество контекста\n",
        "Нет дублирования – одинаковые предложения не повторяются.\n",
        "Контекст строится правильно – previous_line идет в конце, перед ней — логичный поток предыдущих реплик.\n",
        "Фразы осмысленные, например:\n",
        "sql\n",
        "Copy\n",
        "Edit\n",
        "And youre smart, and youre funny but you are bitter. [SEP] And youre lonely, so you treat Everyone around like theyre idiots and you get away with it because of your cane. [SEP] But youre not actually getting away with it.\n",
        "Это логично: сначала общая характеристика, затем последняя реплика, которая ведет к ответу персонажа.\n",
        "✅ Вывод: Теперь контексты выглядят естественно и логично."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqhV3EJnOdgx",
        "outputId": "2a2c1b8f-c103-4642-ac19-d71452b000c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры очищенных `previous_line`:\n",
            "5483                 Dextromethorphan. As in cough syrup?\n",
            "4556    His kidneys are fried. If he doesnt have FMF, ...\n",
            "4056                Why do you care how I feel about her?\n",
            "1811    I pass a farm on my way to school. And theyre ...\n",
            "763                                                 What?\n",
            "Name: previous_line, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def clean_previous_line(text):\n",
        "    \"\"\"Удаляет лишние пробелы и исправляет форматирование в последней реплике\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.strip()\n",
        "\n",
        "# Применение к датасету\n",
        "house_df[\"previous_line\"] = house_df[\"previous_line\"].apply(clean_previous_line)\n",
        "\n",
        "# Проверка примеров\n",
        "print(\"\\nПримеры очищенных `previous_line`:\")\n",
        "print(house_df[\"previous_line\"].sample(5, random_state=42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q54JzT0pOs-L",
        "outputId": "91c19a3e-5011-484d-9eee-975c31d6a6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры исправленных вопросов:\n",
            "                                          previous_line  \\\n",
            "5483               Dextromethorphan. As in cough syrup?   \n",
            "4556  His kidneys are fried. If he doesnt have FMF, ...   \n",
            "4056              Why do you care how I feel about her?   \n",
            "1811  I pass a farm on my way to school. And theyre ...   \n",
            "763                                               What?   \n",
            "\n",
            "                                          final_context  \\\n",
            "5483  Ah, what a shame. [SEP] I meant for you. [SEP]...   \n",
            "4556  Most common cause of anhedonia is sParkzophren...   \n",
            "4056  You need to run a kidney function test. [SEP] ...   \n",
            "1811  I wanna get depo provera. [SEP] Yeah but it wo...   \n",
            "763   Syndrome X could cause a stroke, but I dont kn...   \n",
            "\n",
            "                                               question  \n",
            "5483               Dextromethorphan. As in cough syrup?  \n",
            "4556                  You dont know if well get better?  \n",
            "4056              Why do you care how I feel about her?  \n",
            "1811  I pass a farm on my way to school. And theyre ...  \n",
            "763                      So, can you give me something?  \n"
          ]
        }
      ],
      "source": [
        "def extract_last_question(row):\n",
        "    \"\"\"Извлекает последний осмысленный вопрос из `previous_line` или `final_context`\"\"\"\n",
        "\n",
        "    # Поиск всех предложений в `final_context`\n",
        "    sentences = re.split(r\"(?<=[?.!])\\s+\", row[\"final_context\"])\n",
        "\n",
        "    # Используем `previous_line`, если там есть вопрос\n",
        "    if is_question(row[\"previous_line\"]):\n",
        "        question = row[\"previous_line\"]\n",
        "    else:\n",
        "        # Поиск последнего вопроса в `final_context`\n",
        "        question = None\n",
        "        for sentence in reversed(sentences):\n",
        "            if is_question(sentence):\n",
        "                question = sentence.strip()\n",
        "                break\n",
        "\n",
        "    # Если `question` не заканчивается `?`, ищем альтернативный вариант\n",
        "    if question and not question.strip().endswith(\"?\"):\n",
        "        for sentence in reversed(sentences):\n",
        "            if is_question(sentence) and sentence.strip().endswith(\"?\"):\n",
        "                question = sentence.strip()\n",
        "                break\n",
        "\n",
        "    # Если `question` ≤ 2 слов, ищем более развернутый вариант\n",
        "    if question and len(question.split()) <= 2:\n",
        "        for sentence in reversed(sentences):\n",
        "            if is_question(sentence) and len(sentence.split()) > 2:\n",
        "                question = sentence.strip()\n",
        "                break\n",
        "\n",
        "    # Удаляем `[SEP]` в начале `question`\n",
        "    if question and question.startswith(\"[SEP]\"):\n",
        "        question = question.replace(\"[SEP]\", \"\").strip()\n",
        "\n",
        "    # Если нет нормального вопроса, используем `previous_line`\n",
        "    return question if question else row[\"previous_line\"]\n",
        "\n",
        "# Применяем функцию\n",
        "house_df[\"question\"] = house_df.apply(extract_last_question, axis=1)\n",
        "\n",
        "# Проверяем примеры\n",
        "print(\"\\nПримеры исправленных вопросов:\")\n",
        "print(house_df[[\"previous_line\", \"final_context\", \"question\"]].sample(5, random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQLmGwRaOd2z"
      },
      "source": [
        "## Финальная структура обучающего датасета\n",
        "\n",
        "### **Описание полей**\n",
        "| Колонка        | Описание  | Пример |\n",
        "|---------------|----------|--------|\n",
        "| **`final_context`** | Контекст до последней реплики | `\"And youre smart, and youre funny but you are bitter. [SEP] And youre lonely, so you treat Everyone around like theyre idiots and you get away with it because of your cane.\"` |\n",
        "| **`previous_line`** | Последняя реплика перед ответом (не обрезается) | `\"But youre not actually getting away with it.\"` |\n",
        "| **`question`** | Последний вопрос из `previous_line` или контекста | `\"What do you mean?\"` |\n",
        "| **`answer`** | Реплика Доктора Хауса (правильный ответ) | `\"I'm getting away with it just fine.\"` |\n",
        "| **`label`** | 1 (если ответ правильный), 0 (если неправильный) | `1` или `0` |\n",
        "\n",
        "### **Логика формирования `question`**\n",
        "- Если в `previous_line` есть вопрос → берем его.  \n",
        "- Если в `previous_line` **нет вопроса**, но есть в `final_context` →  \n",
        "  - Берем **последний вопрос** в `final_context`.  \n",
        "- Если в `final_context` **несколько вопросов** → берем **последний** (он самый свежий).  \n",
        "- Если в `final_context` **нет вопросов**, используем `previous_line`.  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWCe_ignSMj_",
        "outputId": "ad60b360-dfb5-4f66-d69c-98c4d3e2b2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры исправленных ответов:\n",
            "                                                   line  \\\n",
            "5483  He wasnt taking it for his cough. Its cheap, a...   \n",
            "4556                           Boy, sure hope Im right.   \n",
            "4056  Because now, I know that I can get you to do a...   \n",
            "1811                   Make love, not belts. Beautiful.   \n",
            "763   I take it you nEver mentioned this during any ...   \n",
            "\n",
            "                                                 answer  \n",
            "5483                  He wasnt taking it for his cough.  \n",
            "4556                           Boy, sure hope Im right.  \n",
            "4056  Because now, I know that I can get you to do a...  \n",
            "1811                              Make love, not belts.  \n",
            "763   I take it you nEver mentioned this during any ...  \n"
          ]
        }
      ],
      "source": [
        "def extract_main_answer(row):\n",
        "    \"\"\"Выделяет главный ответ из `line`, убирая лишние вводные слова и сарказм\"\"\"\n",
        "\n",
        "    text = row[\"line\"]\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    # Если ответ короткий, оставляем его полностью\n",
        "    words = text.split()\n",
        "    if len(words) <= 10:\n",
        "        return text.strip()\n",
        "\n",
        "    # Разделяем на предложения\n",
        "    sentences = re.split(r\"(?<=[?.!])\\s+\", text)\n",
        "\n",
        "    # Используем первое осмысленное предложение\n",
        "    for sentence in sentences:\n",
        "        if len(sentence.split()) > 3:  # Берем осмысленное предложение\n",
        "            return sentence.strip()\n",
        "\n",
        "    # Если ничего не подошло, возвращаем весь ответ\n",
        "    return text.strip()\n",
        "\n",
        "# Применяем функцию\n",
        "house_df[\"answer\"] = house_df.apply(extract_main_answer, axis=1)\n",
        "\n",
        "# Проверяем примеры\n",
        "print(\"\\nПримеры ответов:\")\n",
        "print(house_df[[\"line\", \"answer\"]].sample(5, random_state=42))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSANwuS_YVp2",
        "outputId": "968cc6ef-3120-427c-f401-39ec50dd1824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры исправленных ответов:\n",
            "                                                   line  \\\n",
            "5483  He wasnt taking it for his cough. Its cheap, a...   \n",
            "4556                           Boy, sure hope Im right.   \n",
            "4056  Because now, I know that I can get you to do a...   \n",
            "1811                   Make love, not belts. Beautiful.   \n",
            "763   I take it you nEver mentioned this during any ...   \n",
            "\n",
            "                                                 answer  \n",
            "5483                  He wasnt taking it for his cough.  \n",
            "4556                           Boy, sure hope Im right.  \n",
            "4056  Because now, I know that I can get you to do a...  \n",
            "1811                              Make love, not belts.  \n",
            "763   I take it you nEver mentioned this during any ...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_answer(text):\n",
        "    \"\"\"Очищает `answer` от вводных слов, выбирает осмысленное предложение\"\"\"\n",
        "\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    # Удаление вводных слов\n",
        "    text = re.sub(r\"^(Well|Yeah|Actually|You know|Look|Listen|Right|Oh|Okay|Anyway|So),?\\s+\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Разделение на предложения\n",
        "    sentences = re.split(r\"(?<=[?.!])\\s+\", text)\n",
        "\n",
        "    # Поиск осмысленного ответа (игнорируя сарказм)\n",
        "    for sentence in sentences:\n",
        "        if len(sentence.split()) > 3 and \"yeah\" not in sentence.lower() and \"sure\" not in sentence.lower():\n",
        "            return sentence.strip()\n",
        "\n",
        "    # Если ничего не найдено, возвращается весь `line`\n",
        "    return text.strip()\n",
        "\n",
        "# Применение функции\n",
        "house_df[\"answer\"] = house_df[\"line\"].apply(clean_answer)\n",
        "\n",
        "# Проверка примеров\n",
        "print(\"\\nПримеры исправленных ответов:\")\n",
        "print(house_df[[\"line\", \"answer\"]].sample(5, random_state=42))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-oYQPccX32g",
        "outputId": "5c572403-f96d-4896-f170-6c39cf983aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Обновлены `question`, если они дублировали `previous_line`.\n"
          ]
        }
      ],
      "source": [
        "def update_question_if_duplicate(row):\n",
        "    \"\"\"Заменяет `question`, если он почти идентичен `previous_line`\"\"\"\n",
        "    if row[\"cosine_question_previous\"] > 0.95:  # Если `question` дублирует `previous_line`\n",
        "        sentences = re.split(r\"(?<=[?.!])\\s+\", row[\"final_context\"])  # Разделение контекста\n",
        "        for sentence in reversed(sentences):  # Берем последний вопрос в `final_context`\n",
        "            if is_question(sentence) and sentence.strip() != row[\"previous_line\"]:\n",
        "                return sentence.strip()\n",
        "    return row[\"question\"]\n",
        "\n",
        "# Применение корректировки\n",
        "house_df[\"question\"] = house_df.apply(update_question_if_duplicate, axis=1)\n",
        "\n",
        "print(\"\\nОбновлены `question`, если они дублировали `previous_line`.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKRk1MTkTa1H",
        "outputId": "55600989-5b30-493a-e15d-22e7a769f28b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Распределение длины ответов:\n",
            "count    7417.000000\n",
            "mean        8.457597\n",
            "std         4.704667\n",
            "min         3.000000\n",
            "25%         5.000000\n",
            "50%         7.000000\n",
            "75%        10.000000\n",
            "90%        15.000000\n",
            "95%        18.000000\n",
            "max        37.000000\n",
            "Name: answer_length, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "def get_text_length(text):\n",
        "    \"\"\"Возвращает количество слов в тексте\"\"\"\n",
        "    return len(text.split()) if isinstance(text, str) else 0\n",
        "\n",
        "# Добавляем колонку с длиной ответа\n",
        "house_df[\"answer_length\"] = house_df[\"answer\"].apply(get_text_length)\n",
        "\n",
        "# Анализ распределения длины ответов\n",
        "length_stats = house_df[\"answer_length\"].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])\n",
        "\n",
        "# Вывод статистики по длине ответов\n",
        "print(\"\\nРаспределение длины ответов:\")\n",
        "print(length_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9LiJmqzT14e"
      },
      "source": [
        "Средняя длина (mean): 8.45 слов → нормально, ответы не слишком короткие и не слишком длинные.\n",
        "Максимальная длина (max): 37 слов → нет выбросов, все в разумных пределах.\n",
        "75% ответов ≤ 10 слов, 95% ответов ≤ 18 слов → оптимально для кросс-энкодера.\n",
        "Минимальная длина (min): 3 слова → достаточно, чтобы быть осмысленным ответом.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af0BM4TkSM5v"
      },
      "source": [
        "**Алгоритм формирования ответов Доктора Хауса (answer)**\n",
        "Ответы в line не всегда:\n",
        "- Релевантны – бывают отвлеченные фразы, сарказм, или отсылки.\n",
        "- В начале реплики – могут идти после вводных фраз, саркастических замечаний.\n",
        "- Четко соответствуют вопросу – могут быть ироничными или резкими.\n",
        "Поэтому нужно выделить главную часть ответа, чтобы модель училась правильно их оценивать.\n",
        "\n",
        "Логика формирования answer\n",
        "- Оставляем line, если он короткий и осмысленный\n",
        "- Если в line ≤ 10 слов, используем его полностью.\n",
        "- Если line длинный (> 10 слов), выделяем главный ответ\n",
        "- Ищем первое предложение (до . или ?, если это полноценный ответ).\n",
        "- Если первое предложение очень короткое (≤ 3 слов), ищем следующее осмысленное предложение.\n",
        "- Если в line есть сарказм, выделяем фразу, относящуюся к вопросу\n",
        "- Ищем ключевые слова (yes, no, maybe, because, that's why, obviously, of course и т. д.). Если такие слова есть, оставляем часть ответа после них.\n",
        "- Если ничего не подошло, оставляем line полностью\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy4uJtXSUoRH"
      },
      "source": [
        "**Проверка косинусной близости в данных**\n",
        "\n",
        "Косинусная близость помогает оценить степень схожести между текстами. Проверка проводится для того, чтобы:\n",
        "- Фильтровать слабосвязанные вопросы и ответы – если ответ не имеет отношения к вопросу, его можно исключить.\n",
        "- Проверять `question` и `previous_line` – если они практически идентичны, возможно, `question` не добавляет новой информации и его нужно заменить.\n",
        "- Создавать более осмысленные негативные примеры (`negative_pairs`) – подбор реплик с низкой косинусной близостью позволяет формировать реалистичные отрицательные примеры.\n",
        "\n",
        " **Какие поля сравниваются?**\n",
        "1. `question` ↔ `answer` – проверка, насколько ответ связан с вопросом.\n",
        "2. `question` ↔ `previous_line` – проверка, отличается ли вопрос от последней реплики, чтобы избежать дублирования.\n",
        "\n",
        " **Какие значения считаются нормальными**\n",
        "- `cosine_question_answer` (вопрос и ответ)  \n",
        "  - ысокая близость (≥ 0.7) – ответ хорошо соответствует вопросу.  \n",
        "  - Средняя близость (0.4 – 0.7) – ответ частично связан с вопросом.  \n",
        "  - Низкая близость (< 0.4) – ответ может быть нерелевантным, возможно, его стоит удалить.  \n",
        "\n",
        "- **`cosine_question_previous` (вопрос и предыдущая реплика)**  \n",
        "  - Высокая близость (> 0.9) – `question` дублирует `previous_line`, стоит проверить, нужен ли этот вопрос.  \n",
        "  - Средняя близость (0.5 – 0.9) – `question` логично вытекает из контекста, что является нормальным.  \n",
        "  - Низкая близость (< 0.5) – `question` может быть сформулирован слишком обобщенно или не связан с предыдущей репликой.  \n",
        "\n",
        " **Что делать с результатами**\n",
        "- Если `cosine_question_answer` слишком низкая, можно удалить эти пары.  \n",
        "- Если `cosine_question_previous` слишком высокая, `question` можно заменить на более информативный.  \n",
        "- Для негативных примеров (`negative_pairs`) подбираются ответы с низкой косинусной близостью к вопросу.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoxE8vZlUGVC",
        "outputId": "9a1aa57c-a4a2-4168-e7e4-6369a067500d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Распределение косинусной близости между `question` и `answer`:\n",
            "count    7417.000000\n",
            "mean        0.207865\n",
            "std         0.197324\n",
            "min        -0.134349\n",
            "25%         0.088306\n",
            "50%         0.158947\n",
            "75%         0.262798\n",
            "max         1.000000\n",
            "Name: cosine_question_answer, dtype: float64\n",
            "\n",
            "Распределение косинусной близости между `question` и `previous_line`:\n",
            "count    7417.000000\n",
            "mean        0.765244\n",
            "std         0.373661\n",
            "min        -0.108959\n",
            "25%         0.438442\n",
            "50%         1.000000\n",
            "75%         1.000000\n",
            "max         1.000000\n",
            "Name: cosine_question_previous, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Загружается модель для эмбеддингов\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def encode_texts(texts):\n",
        "    \"\"\"Векторизует список текстов с использованием модели SentenceTransformer\"\"\"\n",
        "    return model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "# Векторизация `question`, `answer`, `previous_line`\n",
        "questions_emb = encode_texts(house_df[\"question\"].tolist())\n",
        "answers_emb = encode_texts(house_df[\"answer\"].tolist())\n",
        "previous_lines_emb = encode_texts(house_df[\"previous_line\"].tolist())\n",
        "\n",
        "# Вычисление косинусной близости\n",
        "house_df[\"cosine_question_answer\"] = [\n",
        "    util.pytorch_cos_sim(questions_emb[i], answers_emb[i]).item() for i in range(len(house_df))\n",
        "]\n",
        "\n",
        "house_df[\"cosine_question_previous\"] = [\n",
        "    util.pytorch_cos_sim(questions_emb[i], previous_lines_emb[i]).item() for i in range(len(house_df))\n",
        "]\n",
        "\n",
        "# Вывод статистики\n",
        "print(\"\\nРаспределение косинусной близости между `question` и `answer`:\")\n",
        "print(house_df[\"cosine_question_answer\"].describe())\n",
        "\n",
        "print(\"\\nРаспределение косинусной близости между `question` и `previous_line`:\")\n",
        "print(house_df[\"cosine_question_previous\"].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3CvBXk3WvUH"
      },
      "source": [
        "Для добавления негативных реплик внесли изменения по сравнению с формированием триплетов для биэнкодера.\n",
        "- Ускорити код с помощью torch.topk() вместо np.argsort().\n",
        "- Считаем top_k не для всех примеров, а только для релевантных.\n",
        "- Добавили фильтрацию по длине, чтобы антагонистический ответ не был намного длиннее или короче вопроса.\n",
        "- Сделали пороги (0.05 < sim < 0.2) параметрами, чтобы настраивать выборку при необходимости."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHFh7NJEYwTw",
        "outputId": "8884935b-accc-4ba9-f19c-eb1c1bb95ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер `questions_emb`: torch.Size([7417, 768])\n",
            "Размер `answers_emb`: torch.Size([7417, 768])\n",
            "\n",
            "Обновленное распределение косинусной близости между `question` и `answer`:\n",
            "count    7417.000000\n",
            "mean        0.188792\n",
            "std         0.192068\n",
            "min        -0.177711\n",
            "25%         0.073482\n",
            "50%         0.141968\n",
            "75%         0.241475\n",
            "max         1.000000\n",
            "Name: cosine_question_answer, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import util\n",
        "\n",
        "# Повторная векторизация `questions_emb` с той же моделью\n",
        "questions_emb = model.encode(house_df[\"question\"].tolist(), convert_to_tensor=True, device=device)\n",
        "\n",
        "# Проверка размерности эмбеддингов\n",
        "print(\"Размер `questions_emb`:\", questions_emb.shape)\n",
        "print(\"Размер `answers_emb`:\", answers_emb.shape)\n",
        "\n",
        "# Пересчет косинусной близости `question` ↔ `answer`\n",
        "house_df[\"cosine_question_answer\"] = [\n",
        "    util.pytorch_cos_sim(questions_emb[i], answers_emb[i]).item() for i in range(len(house_df))\n",
        "]\n",
        "\n",
        "# Вывод обновленной статистики\n",
        "print(\"\\nОбновленное распределение косинусной близости между `question` и `answer`:\")\n",
        "print(house_df[\"cosine_question_answer\"].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQW-Sj8PdYln",
        "outputId": "12438ee9-4418-4e4a-985c-e75571b14528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры с низкой косинусной близостью (`question` ↔ `answer`):\n",
            "                                               question  \\\n",
            "558                             [SEP] Like you are now?   \n",
            "6076       You think her exhusband tried to poison her?   \n",
            "4932  A life where Ill continually drive away anyone...   \n",
            "6589          Youre not signing that for moral reasons?   \n",
            "4732    What if its a cancer syndrome, like Trousseaus?   \n",
            "\n",
            "                                                 answer  \\\n",
            "558   If you do the surgery hes gonna lay on that ta...   \n",
            "6076  And you would have Even it you hadnt told me t...   \n",
            "4932                              Those are the breaks.   \n",
            "6589        I think she looks beautiful the way she is.   \n",
            "4732  And why hes gone three years without anyone se...   \n",
            "\n",
            "      cosine_question_answer  \n",
            "558                 0.058105  \n",
            "6076                0.077848  \n",
            "4932                0.034177  \n",
            "6589                0.009933  \n",
            "4732                0.032084  \n"
          ]
        }
      ],
      "source": [
        "low_similarity_examples = house_df[house_df[\"cosine_question_answer\"] < 0.1].sample(5, random_state=42)\n",
        "print(\"\\nПримеры с низкой косинусной близостью (`question` ↔ `answer`):\")\n",
        "print(low_similarity_examples[[\"question\", \"answer\", \"cosine_question_answer\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_aXpATYdgBv",
        "outputId": "0aab0e8a-88b3-4dcf-85fe-7b743b5dee92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Обновленное распределение косинусной близости (`question` ↔ `answer`):\n",
            "count    7417.000000\n",
            "mean        0.190341\n",
            "std         0.192679\n",
            "min        -0.177711\n",
            "25%         0.075317\n",
            "50%         0.143255\n",
            "75%         0.242321\n",
            "max         1.000000\n",
            "Name: cosine_question_answer, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "def refine_answer(row):\n",
        "    \"\"\"Выбирает более релевантный ответ, если текущий имеет низкую косинусную близость\"\"\"\n",
        "\n",
        "    if row[\"cosine_question_answer\"] < 0.1:  # Если ответ слишком нерелевантен\n",
        "        sentences = re.split(r\"(?<=[?.!])\\s+\", row[\"line\"])  # Разбиваем `line` на предложения\n",
        "\n",
        "        # Исключение саркастичных фраз\n",
        "        sarcasm_markers = [\"never\", \"sure\", \"right\", \"of course\", \"obviously\"]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.split()) > 3 and not any(word in sentence.lower() for word in sarcasm_markers):\n",
        "                return sentence.strip()\n",
        "\n",
        "    return row[\"answer\"]  # Если ничего не найдено, оставляем текущий ответ\n",
        "\n",
        "# Обновляем `answer`\n",
        "house_df[\"answer\"] = house_df.apply(refine_answer, axis=1)\n",
        "\n",
        "# Повторная векторизация `answer`\n",
        "answers_emb = model.encode(house_df[\"answer\"].tolist(), convert_to_tensor=True, device=device)\n",
        "\n",
        "# Повторный пересчет косинусной близости\n",
        "house_df[\"cosine_question_answer\"] = [\n",
        "    util.pytorch_cos_sim(questions_emb[i], answers_emb[i]).item() for i in range(len(house_df))\n",
        "]\n",
        "\n",
        "# Проверка нового распределения\n",
        "print(\"\\nОбновленное распределение косинусной близости (`question` ↔ `answer`):\")\n",
        "print(house_df[\"cosine_question_answer\"].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6y7CkIJe-eX",
        "outputId": "985ee02d-7b57-4901-b65e-228bdb0a6a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Примеры с низкой косинусной близостью (`question` ↔ `answer`):\n",
            "                                      question  \\\n",
            "6523                Doing it I dont know once?   \n",
            "1089  Is that what you medically need to know?   \n",
            "4003           What did you get for Christmas?   \n",
            "2546           [SEP] What are you looking for?   \n",
            "5189                      What does it matter?   \n",
            "\n",
            "                                                 answer  \\\n",
            "6523  Im not gonna act like theres a crisis before w...   \n",
            "1089                   What jewelry did your bride wear   \n",
            "4003                           Its like watParkng some!   \n",
            "2546  Hes telling us what hes seeing, telling us exa...   \n",
            "5189                                I saw guilt in him.   \n",
            "\n",
            "      cosine_question_answer  \n",
            "6523               -0.018382  \n",
            "1089               -0.010579  \n",
            "4003                0.055450  \n",
            "2546                0.087292  \n",
            "5189                0.091719  \n"
          ]
        }
      ],
      "source": [
        "low_similarity_examples = house_df[house_df[\"cosine_question_answer\"] < 0.1].sample(5, random_state=42)\n",
        "print(\"\\nПримеры с низкой косинусной близостью (`question` ↔ `answer`):\")\n",
        "print(low_similarity_examples[[\"question\", \"answer\", \"cosine_question_answer\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guccR3NhfokZ",
        "outputId": "d2c5c546-90a2-4835-af87-cc62d22b8de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Обновленное распределение косинусной близости (`question` ↔ `answer`):\n",
            "count    7417.000000\n",
            "mean        0.190341\n",
            "std         0.192679\n",
            "min        -0.177711\n",
            "25%         0.075317\n",
            "50%         0.143255\n",
            "75%         0.242321\n",
            "max         1.000000\n",
            "Name: cosine_question_answer, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "def refine_answer(row):\n",
        "    \"\"\"Выбирает более релевантный ответ, если текущий имеет низкую косинусную близость\"\"\"\n",
        "\n",
        "    if row[\"cosine_question_answer\"] < 0.1:  # Если ответ слишком нерелевантен\n",
        "        sentences = re.split(r\"(?<=[?.!])\\s+\", row[\"line\"])  # Разбиваем `line` на предложения\n",
        "\n",
        "        # Исключение саркастичных фраз\n",
        "        sarcasm_markers = [\"never\", \"sure\", \"right\", \"of course\", \"obviously\"]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.split()) > 3 and not any(word in sentence.lower() for word in sarcasm_markers):\n",
        "                return sentence.strip()\n",
        "\n",
        "    return row[\"answer\"]  # Если ничего не найдено, оставляем текущий ответ\n",
        "\n",
        "# Обновляем `answer`\n",
        "house_df[\"answer\"] = house_df.apply(refine_answer, axis=1)\n",
        "\n",
        "# Повторная векторизация `answer`\n",
        "answers_emb = model.encode(house_df[\"answer\"].tolist(), convert_to_tensor=True, device=device)\n",
        "\n",
        "# Повторный пересчет косинусной близости\n",
        "house_df[\"cosine_question_answer\"] = [\n",
        "    util.pytorch_cos_sim(questions_emb[i], answers_emb[i]).item() for i in range(len(house_df))\n",
        "]\n",
        "\n",
        "# Проверка нового распределения\n",
        "print(\"\\nОбновленное распределение косинусной близости (`question` ↔ `answer`):\")\n",
        "print(house_df[\"cosine_question_answer\"].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDMJWGQqXN7o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import util\n",
        "\n",
        "# Параметры выбора негативных примеров\n",
        "threshold_lower = 0.05   # Минимальная схожесть (слишком далекое — игнорируется)\n",
        "threshold_upper = 0.2    # Максимальная схожесть (слишком похожее — игнорируется)\n",
        "top_k = 50               # Берем `top_k` лучших кандидатов\n",
        "max_length_diff = 10     # Максимальная разница в длине ответа и антагониста\n",
        "\n",
        "# Векторизация `antagonists` (делается один раз!)\n",
        "antagonist_lines = antagonists_df[\"line\"].tolist()\n",
        "antagonist_embeddings = model.encode(antagonist_lines, convert_to_tensor=True, device=device)\n",
        "\n",
        "# Создание массива для негативных примеров\n",
        "new_hard_negatives = np.full(len(house_df), \"\", dtype=object)\n",
        "\n",
        "# Векторизация `question` и обработка по батчам\n",
        "batch_size = 128  # Можно уменьшать, если GPU перегружен\n",
        "questions = house_df[\"question\"].tolist()\n",
        "\n",
        "for i in range(0, len(questions), batch_size):\n",
        "    batch_questions = questions[i:i+batch_size]  # Берем батч вопросов\n",
        "    batch_embeddings = model.encode(batch_questions, convert_to_tensor=True, device=device)\n",
        "\n",
        "    # Вычисление косинусной близости для батча\n",
        "    batch_similarities = util.pytorch_cos_sim(batch_embeddings, antagonist_embeddings).cpu()\n",
        "\n",
        "    # Обработка каждого примера в батче\n",
        "    for j in range(len(batch_questions)):\n",
        "        question_idx = i + j  # Индекс текущего вопроса в house_df\n",
        "        similarities_row = batch_similarities[j]  # Косинусная близость для этого вопроса\n",
        "\n",
        "        # Фильтрация по порогу схожести\n",
        "        sorted_idx = torch.topk(similarities_row, k=top_k, largest=True).indices.numpy()\n",
        "        candidates = [\n",
        "            idx for idx in sorted_idx\n",
        "            if threshold_lower < similarities_row[idx].item() < threshold_upper\n",
        "        ]\n",
        "\n",
        "        # Дополнительная фильтрация по длине ответа\n",
        "        question_length = len(questions[question_idx].split())\n",
        "        filtered_candidates = [\n",
        "            idx for idx in candidates\n",
        "            if abs(len(antagonist_lines[idx].split()) - question_length) <= max_length_diff\n",
        "        ]\n",
        "\n",
        "        # Выбор случайного негативного примера\n",
        "        if len(filtered_candidates) > 0:\n",
        "            neg_idx = np.random.choice(filtered_candidates)\n",
        "            new_hard_negatives[question_idx] = antagonist_lines[neg_idx]\n",
        "\n",
        "# Добавление негативных примеров в `house_df`\n",
        "house_df[\"neg_answer\"] = new_hard_negatives\n",
        "print(f\"Обновлено {len(new_hard_negatives)} негативных примеров.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvx-MMSSgIqr"
      },
      "outputs": [],
      "source": [
        "# Создание DataFrame с позитивными примерами (`label = 1`)\n",
        "positive_pairs = house_df[[\"final_context\", \"question\", \"answer\"]].copy()\n",
        "positive_pairs[\"label\"] = 1  # Метка правильного ответа\n",
        "\n",
        "# Создание DataFrame с негативными примерами (`label = 0`)\n",
        "negative_pairs = house_df[[\"final_context\", \"question\", \"neg_answer\"]].copy()\n",
        "negative_pairs.rename(columns={\"neg_answer\": \"answer\"}, inplace=True)  # Переименовать колонку для единого формата\n",
        "negative_pairs[\"label\"] = 0  # Метка неправильного ответа\n",
        "\n",
        "# Объединение позитивных и негативных примеров\n",
        "final_dataset = pd.concat([positive_pairs, negative_pairs], ignore_index=True)\n",
        "\n",
        "# Сохранение датасета\n",
        "final_dataset.to_csv(\"cross_encoder_dataset.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Финальный датасет сохранен: {len(final_dataset)} строк.\")\n",
        "print(final_dataset.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj3K8XoBghew"
      },
      "outputs": [],
      "source": [
        "df = final_dataset\n",
        "\n",
        "# Создание списка данных для кросс-энкодера\n",
        "reranker_data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # Добавление позитивного примера (правильный ответ)\n",
        "    reranker_data.append({\"combined\": f\"{row['final_context']} [SEP] {row['question']} [SEP] {row['answer']}\", \"label\": 1})\n",
        "\n",
        "    # Добавление негативного примера (неправильный ответ)\n",
        "    reranker_data.append({\"combined\": f\"{row['final_context']} [SEP] {row['question']} [SEP] {row['neg_answer']}\", \"label\": 0})\n",
        "\n",
        "# Преобразование в DataFrame\n",
        "reranker_df = pd.DataFrame(reranker_data)\n",
        "\n",
        "# Сохранение в `.pkl` для обучения\n",
        "reranker_df.to_pickle(\"reranker_dataset.pkl\")\n",
        "\n",
        "print(f\"Создан датасет для кросс-энкодера: {len(reranker_df)} пар.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
