# -*- coding: utf-8 -*-
"""Datasets_merging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibX7Qr7S8blOVgj8ZbbWoirG1Bt-i3y9

# –í—ã–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
"""

import os
import glob
import pandas as pd
import re
import chardet
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
import nltk

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
nltk.download('stopwords')

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# === –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ===
root_dir = "/content/"
output_dir = os.path.join(root_dir, "cleaned_data")
os.makedirs(output_dir, exist_ok=True)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π ===
characters = {
    # ‚úÖ Dr. House
    "Dr. House": {"folder": "Dr. House", "columns": ["name", "line"], "filter": r"(?i)house"},

    # ‚úÖ Game of Thrones (–†–∞—Å—à–∏—Ä–µ–Ω–æ)
    "Tyrion Lannister": {"folder": "Game of Thrones", "columns": ["Speaker", "Text"], "filter": r"(?i)\bTYRION\b"},
    "Jon Snow": {"folder": "Game of Thrones", "columns": ["Speaker", "Text"], "filter": r"(?i)\bJON\b"},
    "Cersei Lannister": {"folder": "Game of Thrones", "columns": ["Speaker", "Text"], "filter": r"(?i)\bCERSEI\b"},
    "Daenerys Targaryen": {"folder": "Game of Thrones", "columns": ["Speaker", "Text"], "filter": r"(?i)\bDAENERYS\b"},
    "Jaime Lannister": {"folder": "Game of Thrones", "columns": ["Speaker", "Text"], "filter": r"(?i)\bJAIME\b"},

    # ‚úÖ The Big Bang Theory (–†–∞—Å—à–∏—Ä–µ–Ω–æ)
    "Sheldon Cooper": {"folder": "The Big Bang Theory", "columns": ["person_scene", "dialogue"], "filter": r"(?i)Sheldon"},
    "Penny": {"folder": "The Big Bang Theory", "columns": ["person_scene", "dialogue"], "filter": r"(?i)\bpenny\b"},
    "Amy Farrah Fowler": {"folder": "The Big Bang Theory", "columns": ["person_scene", "dialogue"], "filter": r"(?i)\bamy\b"},
    "Leonard Hofstadter": {"folder": "The Big Bang Theory", "columns": ["person_scene", "dialogue"], "filter": r"(?i)\bleonard\b"},
    "Howard Wolowitz": {"folder": "The Big Bang Theory", "columns": ["person_scene", "dialogue"], "filter": r"(?i)\bhoward\b"},
    "Rajesh Koothrappali": {"folder": "The Big Bang Theory", "columns": ["person_scene", "dialogue"], "filter": r"(?i)\brajesh\b"},

    # ‚úÖ The Office (–†–∞—Å—à–∏—Ä–µ–Ω–æ)
    "Michael Scott": {"folder": "The Office", "columns": None, "filter": r"(?i)\bMICHAEL\b"},
    "Dwight Schrute": {"folder": "The Office", "columns": None, "filter": r"(?i)\bDWIGHT\b"},
    "Jim Halpert": {"folder": "The Office", "columns": None, "filter": r"(?i)\bJIM\b"},
    "Pam Beesly": {"folder": "The Office", "columns": None, "filter": r"(?i)\bPAM\b"},
    "Stanley Hudson": {"folder": "The Office", "columns": None, "filter": r"(?i)\bSTANLEY\b"},

    # ‚úÖ Rick and Morty (–†–∞—Å—à–∏—Ä–µ–Ω–æ)
    "Rick Sanchez": {"folder": "Rick&Morty", "columns": ["name", "line"], "filter": r"(?i)^rick$"},
    "Morty Smith": {"folder": "Rick&Morty", "columns": ["name", "line"], "filter": r"(?i)^morty$"},
    "Summer Smith": {"folder": "Rick&Morty", "columns": ["name", "line"], "filter": r"(?i)^summer$"},
    "Beth Smith": {"folder": "Rick&Morty", "columns": ["name", "line"], "filter": r"(?i)^beth$"},
    "Jerry Smith": {"folder": "Rick&Morty", "columns": ["name", "line"], "filter": r"(?i)^jerry$"},

    # ‚úÖ Friends (–†–∞—Å—à–∏—Ä–µ–Ω–æ)
    "Phoebe Buffay": {"folder": "Friends", "columns": ["character", "line"], "filter": r"(?i)\bphoebe\b"},
    "Chandler Bing": {"folder": "Friends", "columns": ["character", "line"], "filter": r"(?i)\bchandler\b"},
    "Monica Geller": {"folder": "Friends", "columns": ["character", "line"], "filter": r"(?i)\bmonica\b"},
    "Ross Geller": {"folder": "Friends", "columns": ["character", "line"], "filter": r"(?i)\bross\b"},
    "Joey Tribbiani": {"folder": "Friends", "columns": ["character", "line"], "filter": r"(?i)\bjoey\b"},
    "Rachel Green": {"folder": "Friends", "columns": ["character", "line"], "filter": r"(?i)\brachel\b"},

    # ‚úÖ Star Wars (–†–∞—Å—à–∏—Ä–µ–Ω–æ)
    "Luke Skywalker": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bLUKE\b"},
    "Han Solo": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bHAN\b"},
    "Darth Vader": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bVADER\b"},
    "Yoda": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bYODA\b"},
    "C-3PO": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bTHREEPIO\b"},
    "Obi-Wan Kenobi": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bOBI-WAN\b"},
    "Princess Leia": {"folder": "Star Wars", "columns": ["character", "line"], "filter": r"(?i)\bLEIA\b"}
}


# === –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö ===

def detect_encoding(file_path):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–∞."""
    with open(file_path, "rb") as f:
        result = chardet.detect(f.read(100000))
        return result.get("encoding", "utf-8")

def load_file(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∏."""
    encoding = detect_encoding(file_path)

    if encoding.lower() in ["ascii", "windows-1252"]:
        encoding = "utf-8-sig"

    try:
        if file_path.endswith(".csv"):
            return pd.read_csv(file_path, encoding=encoding, on_bad_lines="skip")
        elif file_path.endswith(".txt"):
            data = []
            with open(file_path, "r", encoding=encoding, errors="ignore") as file:
                lines = file.readlines()
                for line in lines:
                    line = line.strip()
                    if ": " in line:
                        character, dialogue = line.split(": ", 1)
                        data.append([character.strip(), dialogue.strip()])
            return pd.DataFrame(data, columns=["character", "line"])
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        return None

def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∫–æ–±–æ–∫."""
    text = re.sub(r"\[.*?\]|\(.*?\)", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–º–∞—Ä–æ–∫
    text = re.sub(r"[-\"']", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –∑–Ω–∞–∫–æ–≤
    text = re.sub(r"\s+", " ", text).strip()  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    return text

def process_dataframe(df, character_settings):
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –æ—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö."""

    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–º–∞—Ä–æ–∫
    if "speaker" in df.columns:
        df = df[~df["speaker"].str.contains(r"\{system\}", case=False, na=False)]

    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
    char_column, line_column = character_settings["columns"]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂—É
    df = df[df[char_column].str.match(character_settings["filter"], case=False, na=False)]

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    df[line_column] = df[line_column].apply(clean_text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    df = df.drop_duplicates().dropna(subset=[line_column])

    return df


def save_dataframe(df, filename):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV-—Ñ–∞–π–ª."""
    output_path = os.path.join(output_dir, filename)
    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

def process_character(character, settings):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–ø–ª–∏–∫–∞–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞."""
    print(f"\nüîπ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–∂: {character}")

    folder_path = os.path.join(root_dir, settings["folder"])
    file_paths = [
        f for f in glob.glob(os.path.join(folder_path, "*.csv")) + glob.glob(os.path.join(folder_path, "*.txt"))
        if "cleaned" not in f  # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    ]

    all_lines = []
    total_lines_before = 0

    for file_path in file_paths:
        print(f"üìÇ –ß–∏—Ç–∞–µ—Ç—Å—è —Ñ–∞–π–ª: {file_path}")
        df = load_file(file_path)

        if df is None:
            continue

        if settings["columns"]:
            missing_cols = [col for col in settings["columns"] if col not in df.columns]
            if missing_cols:
                print(f"‚ùå –û—à–∏–±–∫–∞: –í {file_path} –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ {missing_cols}")
                continue

            lines = df[df[settings["columns"][0]].astype(str).fillna("").str.contains(settings["filter"], case=False, na=False)][settings["columns"][1]].dropna().tolist()
        else:
            lines = df[df["character"].str.match(settings["filter"], case=False, na=False)]["line"].dropna().tolist()

        total_lines_before += len(lines)
        lines = [clean_text(line) for line in lines if len(line.split()) > 2]
        all_lines.extend(lines)

    all_lines = list(set(all_lines))
    total_unique_lines = len(all_lines)

    if all_lines:
        df_cleaned = pd.DataFrame({"character": character, "line": all_lines})
        save_dataframe(df_cleaned, f"{character.lower().replace(' ', '_')}_lines_cleaned.csv")
    else:
        print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character}")

# === –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π ===
for char, settings in characters.items():
    process_character(char, settings)

print("\nüéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ cleaned_data/")

# –ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ SpongeBob
spongebob_folder = os.path.join(root_dir, "SpongeBob")
output_folder = os.path.join(root_dir, "cleaned_data")

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
os.makedirs(output_folder, exist_ok=True)

file_path = os.path.join(spongebob_folder, "dataset.csv")

# –°–ø–∏—Å–æ–∫ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä—ã—Ö –º—ã —Ö–æ—Ç–∏–º –∏–∑–≤–ª–µ—á—å
spongebob_characters = ["SpongeBob", "Patrick", "Squidward", "Mr. Krabs", "Sandy"]

def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Ä–µ–º–∞—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
    if pd.isna(text):
        return ""  # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
    text = re.sub(r"\[.*?\]|\(.*?\)", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–º–∞—Ä–æ–∫
    text = re.sub(r"[-\"']", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –∑–Ω–∞–∫–æ–≤
    text = re.sub(r"\s+", " ", text).strip()  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    return text

def process_character(file_path, character):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ SpongeBob."""
    print(f"\nüîπ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–∂: {character}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
    try:
        df = pd.read_csv(file_path, encoding="ISO-8859-1", sep=";", on_bad_lines="skip")
        print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω! –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_columns = ["speaker", "replica"]
    if not all(col in df.columns for col in required_columns):
        print(f"‚ùå –û—à–∏–±–∫–∞: –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ {required_columns}")
        return

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂—É
    df = df[df["speaker"].str.strip().eq(character)].copy()

    # –ü–æ–¥—Å—á–µ—Ç —Ñ—Ä–∞–∑ –¥–æ –æ—á–∏—Å—Ç–∫–∏
    num_lines_before = len(df)

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    df["replica"] = df["replica"].apply(clean_text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑ (–º–µ–Ω–µ–µ 3 —Å–ª–æ–≤)
    df = df[df["replica"].str.count(r'\w+') >= 3]

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    df = df.drop_duplicates(subset=["replica"])
    num_duplicates_removed = num_lines_before - len(df)

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    df = df.rename(columns={"speaker": "character", "replica": "line"})

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    df = df[["character", "line"]]

    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–∑ (–¥–æ –æ—á–∏—Å—Ç–∫–∏): {num_lines_before}")
    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {num_duplicates_removed}")
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑: {len(df)}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    output_path = os.path.join(output_folder, f"{character.lower().replace(' ', '_')}_cleaned.csv")
    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

# –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
for character in spongebob_characters:
    process_character(file_path, character)

# –ü–∞–ø–∫–∞ —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
cleaned_data_folder = "/content/drive/MyDrive/–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/–ú–§–¢–ò/2 –∫—É—Ä—Å/–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ NLP/Task 1/cleaned_data"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞
if not os.path.exists(cleaned_data_folder):
    raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ {cleaned_data_folder} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
files = [f for f in os.listdir(cleaned_data_folder) if f.endswith("_cleaned.csv")]

if not files:
    raise FileNotFoundError(f"‚ùå –í –ø–∞–ø–∫–µ {cleaned_data_folder} –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")

print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤: {files}")

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
character_dfs = {}

# –î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats_data = []

# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã
for file in files:
    file_path = os.path.join(cleaned_data_folder, file)

    try:
        df = pd.read_csv(file_path)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if "character" in df.columns and "line" in df.columns:
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫
            num_lines = len(df)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å
            character_name = file.replace("_cleaned.csv", "").replace("_", " ").title()
            character_dfs[character_name] = df

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats_data.append({"Character": character_name, "Total Lines": num_lines})

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {num_lines} —Ä–µ–ø–ª–∏–∫ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character_name}")

        else:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª {file}: –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file}: {e}")

# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
stats_df = pd.DataFrame(stats_data).sort_values(by="Total Lines", ascending=False)

# –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats_df

# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã
files = [f for f in os.listdir(root_dir) if f.endswith("_cleaned.csv")]

if not files:
    raise FileNotFoundError(f"‚ùå –í –ø–∞–ø–∫–µ {root_dir} –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤!")

print(f"\nüìÇ ***–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤***")
print(files)  # –í—ã–≤–æ–¥ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

# === –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö ===
def load_and_clean_data(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV, —É–¥–∞–ª—è–µ—Ç NaN, –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path}: {e}")
        return None

    num_before = len(df)

    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    df.dropna(subset=["line"], inplace=True)
    df.drop_duplicates(subset=["line"], inplace=True)

    num_after = len(df)
    num_removed = num_before - num_after

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    df["line"] = df["line"].apply(lambda x: re.sub(r'\s+', ' ', str(x).strip()))

    return df, num_before, num_removed

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
data_dict = {}
stats = []

print("\nüìÇ ***–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π...***")
for file in files:
    file_path = os.path.join(root_dir, file)
    character = file.replace("_cleaned.csv", "").replace("_", " ").title()  # –ò–º—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞

    df, num_before, num_removed = load_and_clean_data(file_path)

    if df is None or len(df) == 0:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –ø–µ—Ä—Å–æ–Ω–∞–∂ {character} (—Ñ–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω)")
        continue

    data_dict[character] = df
    stats.append((character, num_before, num_removed, len(df)))

    print(f"‚úÖ {character}: {num_before} ‚Üí {len(df)} (–£–¥–∞–ª–µ–Ω–æ: {num_removed})")

if not data_dict:
    raise ValueError("‚ùå –û—à–∏–±–∫–∞! –ù–∏ –æ–¥–∏–Ω –ø–µ—Ä—Å–æ–Ω–∞–∂ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è. –ü—Ä–æ–≤–µ—Ä—å –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤.")

# === –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –î–æ–∫—Ç–æ—Ä–æ–º –•–∞—É—Å–æ–º ===
def compute_similarity(character_1, character_2, data_dict):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤—É—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É"""
    if character_1 not in data_dict or character_2 not in data_dict:
        return None

    # –ë–µ—Ä–µ–º 100 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫ –æ—Ç –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
    lines_1 = data_dict[character_1]["line"].sample(min(100, len(data_dict[character_1]))).tolist()
    lines_2 = data_dict[character_2]["line"].sample(min(100, len(data_dict[character_2]))).tolist()

    # –ö–æ–¥–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏
    with torch.no_grad():
        embeddings_1 = model(**tokenizer(lines_1, return_tensors="pt", padding=True, truncation=True, max_length=256))
        embeddings_2 = model(**tokenizer(lines_2, return_tensors="pt", padding=True, truncation=True, max_length=256))

    # –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    emb_1 = embeddings_1.last_hidden_state.mean(dim=1)
    emb_2 = embeddings_2.last_hidden_state.mean(dim=1)

    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    return pytorch_cos_sim(emb_1.mean(dim=0), emb_2.mean(dim=0)).item()

# === –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –î–æ–∫—Ç–æ—Ä–æ–º –•–∞—É—Å–æ–º ===
similarity_scores = []

print("\nüîç ***–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –î–æ–∫—Ç–æ—Ä–æ–º –•–∞—É—Å–æ–º...***")
for character in data_dict.keys():
    if character == "Dr. House":
        continue

    sim = compute_similarity("Dr. House", character, data_dict)
    if sim is not None:
        similarity_scores.append((character, sim))

# –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
similarity_scores.sort(key=lambda x: x[1], reverse=True)

# === –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
similarity_df = pd.DataFrame(similarity_scores, columns=["–ü–µ—Ä—Å–æ–Ω–∞–∂", "–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –•–∞—É—Å–æ–º"])
stats_df = pd.DataFrame(stats, columns=["–ü–µ—Ä—Å–æ–Ω–∞–∂", "–ò—Å—Ö–æ–¥–Ω—ã—Ö —Ñ—Ä–∞–∑", "–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤", "–û—Å—Ç–∞–ª–æ—Å—å —Ñ—Ä–∞–∑"])

print("\nüîç ***–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –î–æ–∫—Ç–æ—Ä–æ–º –•–∞—É—Å–æ–º***")
print(similarity_df)

print("\nüìä ***–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º***")
print(stats_df)

from sentence_transformers import SentenceTransformer, util
import os
import pandas as pd
import torch

# –ó–∞–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ ===
files = [f for f in os.listdir(root_dir) if f.endswith("_cleaned.csv")]

if not files:
    raise FileNotFoundError(f"‚ùå –í –ø–∞–ø–∫–µ {root_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤!")

print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤: {files}")

# === –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞ ===
house_file = next((os.path.join(root_dir, f) for f in files if "house" in f.lower()), None)

if not house_file:
    raise FileNotFoundError("‚ùå –§–∞–π–ª —Å —Ä–µ–ø–ª–∏–∫–∞–º–∏ –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω!")

print(f"üìå –§–∞–π–ª Dr. House: {house_file}")

# === –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–ø–ª–∏–∫–∏ Dr. House ===
df_house = pd.read_csv(house_file).dropna().drop_duplicates()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–µ
if df_house.empty:
    raise ValueError("‚ùå –§–∞–π–ª —Å —Ä–µ–ø–ª–∏–∫–∞–º–∏ –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞ –ø—É—Å—Ç–æ–π!")

# –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —Ä–µ–ø–ª–∏–∫ –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞
house_embeddings = model.encode(df_house["line"].tolist(), convert_to_tensor=True)

# === –ü–æ–¥—Å—á–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ ===
similarity_scores = {}

for file in files:
    file_path = os.path.join(root_dir, file)

    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º —Ñ–∞–π–ª –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞
    if file_path == house_file:
        continue

    try:
        df = pd.read_csv(file_path).dropna().drop_duplicates()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file}: {e}")
        continue

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–µ
    if df.empty:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {file}, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –ø—É—Å—Ç–æ–π.")
        continue

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    character = " ".join(file.split("_")[:2]).replace(".csv", "").capitalize()

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —Ä–µ–ø–ª–∏–∫ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
    char_embeddings = model.encode(df["line"].tolist(), convert_to_tensor=True)

    # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–ø–æ —Ä–µ–ø–ª–∏–∫–∞–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞)
    similarities = util.pytorch_cos_sim(char_embeddings, house_embeddings)

    # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ **–æ—Å–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞** (—ç—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –º–µ—Ç–æ–¥)
    mean_similarity = similarities.mean(dim=1).mean().item()
    std_similarity = similarities.mean(dim=1).std().item()
    min_similarity = similarities.min().item()
    max_similarity = similarities.max().item()

    similarity_scores[character] = (mean_similarity, std_similarity, min_similarity, max_similarity)

    print(f"üìä {character}: mean = {mean_similarity:.4f}, std = {std_similarity:.4f}, min = {min_similarity:.4f}, max = {max_similarity:.4f}")

# === –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1][0], reverse=True)

print("\nüîç ***–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –î–æ–∫—Ç–æ—Ä–æ–º –•–∞—É—Å–æ–º***")
for character, (mean_sim, std_sim, min_sim, max_sim) in sorted_scores:
    print(f"{character}: mean = {mean_sim:.4f}, std = {std_sim:.4f}, min = {min_sim:.4f}, max = {max_sim:.4f}")

print("\nüéØ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏–Ω—è—Ç—å –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π.")

"""# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import glob
import chardet
import re

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
root_dir = "/content/drive/MyDrive/–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/–ú–§–¢–ò/2 –∫—É—Ä—Å/–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ NLP/Task 1/Dr. House/"
all_files = glob.glob(root_dir + "season*.csv")
max_context = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

# === –ß—Ç–µ–Ω–∏–µ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ ===
df_list = []

for file in all_files:
    with open(file, "rb") as f:
        encoding = chardet.detect(f.read(100000))["encoding"]

    print(f"üìÇ –ß–∏—Ç–∞–µ—Ç—Å—è —Ñ–∞–π–ª: {file} —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}")

    try:
        df = pd.read_csv(file, encoding=encoding)
        df["season"] = re.search(r"season(\d+)", file).group(1)  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Å–µ–∑–æ–Ω–∞
        df_list.append(df)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file}: {e}")

if not df_list:
    raise ValueError("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª!")

df = pd.concat(df_list, ignore_index=True)

# === –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ===
def clean_text(text):
    """–£–¥–∞–ª—è–µ—Ç —Ä–µ–º–∞—Ä–∫–∏, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –∑–Ω–∞–∫–∏"""
    text = re.sub(r"\[.*?\]|\(.*?\)", "", str(text))  # –£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–º–∞—Ä–æ–∫
    text = re.sub(r"[-\"']", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –∑–Ω–∞–∫–æ–≤
    text = re.sub(r"\s+", " ", text).strip()  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    return text

df["name"] = df["name"].astype(str).str.strip().str.lower()
df["line"] = df["line"].apply(clean_text)

# === –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ä–µ–ø–ª–∏–∫–∏ –∏ —Å–ø–∏–∫–µ—Ä–∞ ===
df["previous_line"] = df["line"].shift(1)
df["previous_speaker"] = df["name"].shift(1)

# === –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ===
df["context_long"] = df["line"]
for i in range(1, max_context + 1):
    df["context_long"] = df["previous_line"].shift(i) + " " + df["context_long"]

# === –£–±–∏—Ä–∞–µ–º NaN –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ===
df.fillna("", inplace=True)

# === –†–∞–∑–º–µ—Ç–∫–∞ —Ä–µ–ø–ª–∏–∫ ===
df["is_greeting"] = df["line"].str.lower().str.startswith(("hello", "hi", "hey", "good morning", "good evening"))
df["is_question"] = df["line"].str.endswith("?")
df["is_negation"] = df["line"].str.contains(r"\bno\b|\bnot\b|\bnever\b", case=False, regex=True)
df["is_exclamation"] = df["line"].str.contains("!")
df["is_sarcasm"] = df["line"].str.contains(r"\boh\b|\bsure\b|\byeah right\b|\bof course\b|\bobviously\b", case=False, regex=True)
df["is_answer"] = df["previous_line"].str.endswith("?")

# === –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞ ===
house_lines = df[df["name"].str.contains("house", case=False)].copy()

# === –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã ===
for i in range(1, max_context + 1):
    house_lines[f"context_{i}"] = house_lines["line"]
    for j in range(1, i + 1):
        house_lines[f"context_{i}"] = house_lines["previous_line"].shift(j) + " " + house_lines[f"context_{i}"]

house_lines.fillna("", inplace=True)

# === –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫ ===
stop_phrases = ["yes", "no", "okay", "uh-huh", "hmm", "..."]
house_lines = house_lines[~house_lines["line"].str.lower().isin(stop_phrases)]
house_lines = house_lines[(house_lines["line"].str.len() > 3) | house_lines["is_answer"]]

print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(house_lines)} —Ä–µ–ø–ª–∏–∫")

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
output_path = f"{root_dir}/house_with_clean_context.csv"
house_lines.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"\n‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

house_lines

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–∞–º–æ–º—É —Å–µ–±–µ
house_lines["is_self_question"] = (house_lines["previous_speaker"] == "house") & house_lines["previous_line"].str.endswith("?")

# –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–µ–ø–ª–∏–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
stats = {
    "–í—Å–µ–≥–æ —Ä–µ–ø–ª–∏–∫": len(house_lines),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤": house_lines["is_answer"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤": house_lines["is_question"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–∞–º–æ–º—É —Å–µ–±–µ": house_lines["is_self_question"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Ä–∏—Ü–∞–Ω–∏–π": house_lines["is_negation"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞—Ä–∫–∞–∑–º–∞": house_lines["is_sarcasm"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫": house_lines["is_exclamation"].sum(),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
stats_df = pd.DataFrame(list(stats.items()), columns=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"])

# –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats_df

import pandas as pd
import re

# === –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏ ===
def split_into_chunks(text, max_words=25):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤."""
    if not isinstance(text, str) or len(text) == 0:
        return []

    sentences = re.split(r'([.!?])\s+', text)  # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    sentences = ["".join(sentences[i:i+2]) for i in range(0, len(sentences), 2)]

    chunks = []
    buffer = []

    for sentence in sentences:
        words = sentence.split()

        if len(words) > max_words:
            for i in range(0, len(words), max_words):
                chunks.append(" ".join(words[i:i+max_words]))
        else:
            buffer.extend(words)
            if len(buffer) > max_words:
                chunks.append(" ".join(buffer))
                buffer = []

    if buffer:
        chunks.append(" ".join(buffer))

    return chunks

# === –†–∞–∑–±–∏–≤–∞–µ–º –¢–û–õ–¨–ö–û –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã ===
house_lines["split_needed"] = house_lines["is_answer"] & (house_lines["line"].str.split().str.len() > 25)

# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ –Ω—É–∂–Ω—ã–º —Å—Ç—Ä–æ–∫–∞–º
house_lines["line_chunks"] = house_lines.apply(lambda row: split_into_chunks(row["line"]) if row["split_needed"] else [row["line"]], axis=1)

# –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
house_lines_expanded = house_lines.explode("line_chunks").drop(columns=["line", "split_needed"]).rename(columns={"line_chunks": "line"})

# –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
house_lines_expanded = house_lines_expanded.dropna(subset=["line"])
house_lines_expanded = house_lines_expanded[house_lines_expanded["line"].str.strip() != ""]

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
output_path = "/content/drive/MyDrive/–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/–ú–§–¢–ò/2 –∫—É—Ä—Å/–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ NLP/Task 1/Dr. House/house_chunks_dataset.csv"
house_lines_expanded.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"\n‚úÖ –†–∞–∑–±–∏—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
print(f"üìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(house_lines_expanded)}")

# –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–µ–ø–ª–∏–∫ –≤ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
stats = {
    "–í—Å–µ–≥–æ —Ä–µ–ø–ª–∏–∫": len(house_lines_expanded),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤": house_lines_expanded["is_answer"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤": house_lines_expanded["is_question"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–∞–º–æ–º—É —Å–µ–±–µ": ((house_lines_expanded["is_question"]) &
                                         (house_lines_expanded["previous_speaker"] == "house")).sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Ä–∏—Ü–∞–Ω–∏–π": house_lines_expanded["is_negation"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞—Ä–∫–∞–∑–º–∞": house_lines_expanded["is_sarcasm"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫": house_lines_expanded["is_exclamation"].sum(),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
stats_df = pd.DataFrame(list(stats.items()), columns=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"])
stats_df

# –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
answers_count = house_lines_expanded["is_answer"].sum()
sarcasm_count = house_lines_expanded["is_sarcasm"].sum()
self_questions_count = house_lines_expanded["is_self_question"].sum()

# –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–ø–ª–∏–∫, —É –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
prev_has_question_mark_count = house_lines_expanded["previous_line"].str.contains(r"\?", na=False).sum()
prev_has_exclamation_mark_count = house_lines_expanded["previous_line"].str.contains(r"!", na=False).sum()

# –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
filtered_lines = house_lines_expanded[
    house_lines_expanded["is_answer"] |
    house_lines_expanded["is_sarcasm"] |
    house_lines_expanded["is_self_question"] |
    house_lines_expanded["previous_line"].str.contains(r"\?", na=False) |
    house_lines_expanded["previous_line"].str.contains(r"!", na=False)
]

total_filtered_count = len(filtered_lines)

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
stats_df = pd.DataFrame({
    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": [
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞—Ä–∫–∞–∑–º–∞",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–∞–º–æ–º—É —Å–µ–±–µ",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –ø–æ—Å–ª–µ –≤–æ–ø—Ä–æ—Å–æ–≤",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –ø–æ—Å–ª–µ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è",
        "–ò—Ç–æ–≥–æ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)"
    ],
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ": [
        answers_count,
        sarcasm_count,
        self_questions_count,
        prev_has_question_mark_count,
        prev_has_exclamation_mark_count,
        total_filtered_count
    ]
})

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
stats_df

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–∫–∂–µ —Ä–∞–∑–º–µ—á–µ–Ω—ã –∫–∞–∫ —Å–∞—Ä–∫–∞–∑–º
questions_with_sarcasm_count = house_lines_expanded[
    (house_lines_expanded["is_question"]) & (house_lines_expanded["is_sarcasm"])
].shape[0]

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
questions_with_sarcasm_count

# –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–∫–∂–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
num_questions_as_answers = house_lines_expanded[(house_lines_expanded["is_question"]) & (house_lines_expanded["is_answer"])].shape[0]

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
num_questions_as_answers

# –£–¥–∞–ª–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ —É –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–ª–∏ —Å–∞—Ä–∫–∞—Å—Ç–∏—á–Ω—ã
def remove_question_mark(text):
    return text.rstrip('?') if text.endswith('?') else text

mask = (house_lines_expanded["is_answer"] & house_lines_expanded["is_question"]) | (house_lines_expanded["is_sarcasm"] & house_lines_expanded["is_question"])
house_lines_expanded.loc[mask, "line"] = house_lines_expanded.loc[mask, "line"].apply(remove_question_mark)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {house_lines_expanded['is_question'].sum()}")
print(house_lines_expanded[mask][["line", "is_answer", "is_sarcasm", "is_question"]].head(10))

# –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ (house_lines_expanded_filtered)
stats = {
    "–í—Å–µ–≥–æ —Ä–µ–ø–ª–∏–∫": len(filtered_lines),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤": filtered_lines["is_answer"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞—Ä–∫–∞–∑–º–∞": filtered_lines["is_sarcasm"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–∞–º–æ–º—É —Å–µ–±–µ": filtered_lines["is_self_question"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –ø–æ—Å–ª–µ –≤–æ–ø—Ä–æ—Å–æ–≤": filtered_lines["previous_line"].str.contains(r"\?", regex=True).sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –ø–æ—Å–ª–µ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è": filtered_lines["previous_line"].str.contains(r"!", regex=True).sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤": filtered_lines["is_question"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Ä–∏—Ü–∞–Ω–∏–π": filtered_lines["is_negation"].sum(),
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫": filtered_lines["is_exclamation"].sum(),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
stats_df = pd.DataFrame(list(stats.items()), columns=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"])

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats_df

filtered_lines

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –î–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞
final_output_path = "/content/drive/MyDrive/–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/–ú–§–¢–ò/2 –∫—É—Ä—Å/–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ NLP/Task 1/cleaned_data/house_final_dataset.csv"
filtered_lines.to_csv(final_output_path, index=False, encoding="utf-8-sig")

# –í—ã–≤–æ–¥ –ø—É—Ç–∏ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
final_output_path

filtered_lines

"""# –í—ã–±–æ—Ä –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫"""

# –î–∞–Ω–Ω—ã–µ –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ä–µ–ø–ª–∏–∫ –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ (–∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –≤—ã–≤–æ–¥–∞)
character_lines = {
    "Daenerys Targaryen": 876, "Jon Snow": 946, "Tyrion Lannister": 1597, "Cersei Lannister": 906, "Jaime Lannister": 810,
    "Dwight Schrute": 6221, "Stanley Hudson": 625, "Pam Beesly": 4117, "Jim Halpert": 5139, "Michael Scott": 10041,
    "Howard Wolowitz": 4989, "Leonard Hofstadter": 7920, "Amy Farrah Fowler": 2928, "Penny": 6121, "Sheldon Cooper": 10318,
    "Spongebob": 1106, "Squidward": 388, "Patrick": 342, "Chandler Bing": 6899, "Joey Tribbiani": 5012, "Monica Geller": 6800,
    "Ross Geller": 7500, "Rachel Green": 7200, "Rick Sanchez": 8210, "Morty Smith": 7354
}

# –ö–æ—Å–∏–Ω—É—Å–Ω—ã–µ –±–ª–∏–∑–æ—Å—Ç–∏ (–æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É)
similarity_scores = {
    "Daenerys Targaryen": 0.0780, "Jon Snow": 0.0789, "Tyrion Lannister": 0.0806, "Cersei Lannister": 0.0821, "Jaime Lannister": 0.0830,
    "Dwight Schrute": 0.0842, "Stanley Hudson": 0.0844, "Pam Beesly": 0.0848, "Jim Halpert": 0.0857, "Michael Scott": 0.0863,
    "Howard Wolowitz": 0.0869, "Leonard Hofstadter": 0.0890, "Amy Farrah Fowler": 0.0886, "Penny": 0.0911, "Sheldon Cooper": 0.0856,
    "Spongebob": 0.0883, "Squidward": 0.0903, "Patrick": 0.0840, "Chandler Bing": 0.0879, "Joey Tribbiani": 0.0877, "Monica Geller": 0.0874,
    "Ross Geller": 0.
}

# –û—Ç–±–∏—Ä–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, —É –∫–æ—Ç–æ—Ä—ã—Ö ‚â• 390 —Ä–µ–ø–ª–∏–∫
selected_characters = [char for char, sim in sorted(similarity_scores.items(), key=lambda x: x[1]) if character_lines[char] >= 390]

print(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_characters)} –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {selected_characters}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –∞–Ω—Ç–∞–≥–æ–Ω–∏—Å—Ç–æ–≤
antagonist_lines = []

for character in selected_characters:
    file_path = f"/content/cleaned_data/{character.lower().replace(' ', '_')}_lines_cleaned.csv"
    df = pd.read_csv(file_path)

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É –∫ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É —Ç–∏–ø—É –∏ –∑–∞–º–µ–Ω—è–µ–º NaN
    df["line"] = df["line"].astype(str).fillna("").str.strip()

    # –£–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å `?` –≤ –∫–æ–Ω—Ü–µ)
    df = df[~df["line"].str.match(r"^[^.!?]+\?$")]

    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    df = df[df["line"] != ""]

    antagonist_lines.append(df)

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
antagonist_dataset = pd.concat(antagonist_lines, ignore_index=True)

# –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats = {"–í—Å–µ–≥–æ —Ä–µ–ø–ª–∏–∫": len(antagonist_dataset)}
for character in selected_characters:
    stats[character] = len(antagonist_dataset[antagonist_dataset["character"] == character])

stats_df = pd.DataFrame(list(stats.items()), columns=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"])

# –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats_df

import pandas as pd
import re

# === –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏ ===
def split_into_chunks(text, max_words=25):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤."""
    if not isinstance(text, str) or len(text) == 0:
        return []

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (—É—á–∏—Ç—ã–≤–∞–µ—Ç `.` `!` `?`)
    sentences = re.split(r'([.!?])\s+', text)
    sentences = ["".join(sentences[i:i+2]) for i in range(0, len(sentences), 2)]

    chunks = []
    buffer = []

    for sentence in sentences:
        words = sentence.split()

        if len(words) > max_words:
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –º–∞–∫—Å. –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤
            for i in range(0, len(words), max_words):
                chunks.append(" ".join(words[i:i+max_words]))
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –±—É—Ñ–µ—Ä
            buffer.extend(words)
            if len(buffer) > max_words:
                chunks.append(" ".join(buffer))
                buffer = []

    # –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ü–µ –±—É—Ñ–µ—Ä –Ω–µ –ø—É—Å—Ç–æ–π, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —á–∞–Ω–∫
    if buffer:
        chunks.append(" ".join(buffer))

    return chunks

# === –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ø–ª–∏–∫ –Ω–∞ —á–∞–Ω–∫–∏ ===
antagonist_dataset["split_needed"] = antagonist_dataset["line"].str.split().str.len() > 25
antagonist_dataset["line_chunks"] = antagonist_dataset.apply(
    lambda row: split_into_chunks(row["line"]) if row["split_needed"] else [row["line"]],
    axis=1
)

# –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
antagonist_dataset_expanded = antagonist_dataset.explode("line_chunks").drop(columns=["line", "split_needed"]).rename(columns={"line_chunks": "line"})

# –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
antagonist_dataset_expanded = antagonist_dataset_expanded.dropna(subset=["line"])

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ===
output_path = "/content/Dr. House/antagonists_chunks_dataset.csv"
antagonist_dataset_expanded.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"\n‚úÖ –†–∞–∑–±–∏—Ç—ã –¥–ª–∏–Ω–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏ –∞–Ω—Ç–∞–≥–æ–Ω–∏—Å—Ç–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
print(f"üìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(antagonist_dataset_expanded)}")

pip install fuzzywuzzy

import pandas as pd

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∞–Ω—Ç–∞–≥–æ–Ω–∏—Å—Ç–æ–≤
file_path = "/content/Dr. House/antagonists_chunks_dataset.csv"
antagonists_df = pd.read_csv(file_path)

# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏
antagonists_df = antagonists_df.dropna(subset=["line"])

# –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–ø–ª–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
character_counts = antagonists_df["character"].value_counts()

# –ü–æ–¥—Å—á–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ (–º–µ–Ω–µ–µ 10 —Å–∏–º–≤–æ–ª–æ–≤)
short_lines = antagonists_df[antagonists_df["line"].str.len() < 10]["character"].value_counts()

# –ü–æ–¥—Å—á–µ—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –æ–¥–Ω—É —Ñ—Ä–∞–∑—É (—Ä–µ–ø–ª–∏–∫–∏, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–µ—Å—è `?`, –Ω–æ –±–µ–∑ –¥—Ä—É–≥–∏—Ö –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
antagonists_df["line"] = antagonists_df["line"].astype(str)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
single_sentence_questions = antagonists_df[antagonists_df["line"].str.match(r"^[^.!?]+\?$", na=False)]["character"].value_counts()

# –ü–æ–¥—Å—á–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ä–µ–ø–ª–∏–∫
duplicate_lines = antagonists_df.duplicated(subset=["line"], keep=False)
duplicate_counts = antagonists_df[duplicate_lines]["character"].value_counts()

# –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
print("üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞:")
print(character_counts)

print("\nüìè –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ (–º–µ–Ω–µ–µ 10 —Å–∏–º–≤–æ–ª–æ–≤):")
print(short_lines)

print("\n‚ùì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –æ–¥–Ω—É —Ñ—Ä–∞–∑—É:")
print(single_sentence_questions)

print("\nüîÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ä–µ–ø–ª–∏–∫:")
print(duplicate_counts)

import pandas as pd

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∞–Ω—Ç–∞–≥–æ–Ω–∏—Å—Ç–æ–≤
antagonists_df = pd.read_csv(file_path)

# –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
antagonists_df = antagonists_df.dropna(subset=["line"])

# –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ (–º–µ–Ω–µ–µ 10 —Å–∏–º–≤–æ–ª–æ–≤)
mask_short = antagonists_df["line"].str.len() < 10
short_replicas_count = antagonists_df.loc[mask_short, "character"].value_counts()
antagonists_df = antagonists_df[~mask_short]

# –£–¥–∞–ª–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –æ–¥–Ω—É —Ñ—Ä–∞–∑—É (—Ä–µ–ø–ª–∏–∫–∏, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–µ—Å—è `?`, –Ω–æ –±–µ–∑ –¥—Ä—É–≥–∏—Ö –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
mask_single_sentence_question = antagonists_df["line"].str.match(r"^[^.!?]+\?$")
single_sentence_question_count = antagonists_df.loc[mask_single_sentence_question, "character"].value_counts()
antagonists_df = antagonists_df[~mask_single_sentence_question]

# –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ä–µ–ø–ª–∏–∫
mask_duplicates = antagonists_df.duplicated(subset=["line"])
duplicate_replicas_count = antagonists_df.loc[mask_duplicates, "character"].value_counts()
antagonists_df = antagonists_df[~mask_duplicates]

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–ø–ª–∏–∫
updated_counts = antagonists_df["character"].value_counts()
total_lines = len(antagonists_df)

# –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
print("üìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:", total_lines)
print("\nüìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ —É –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
print(updated_counts)

# –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —É–¥–∞–ª–µ–Ω–Ω—ã–º —Ä–µ–ø–ª–∏–∫–∞–º
print("\n‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ (<10 —Å–∏–º–≤–æ–ª–æ–≤) –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º:")
print(short_replicas_count)

print("\n‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –æ–¥–Ω–æ—Ñ—Ä–∞–∑–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º:")
print(single_sentence_question_count)

print("\n‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ä–µ–ø–ª–∏–∫ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º:")
print(duplicate_replicas_count)

import pandas as pd

# –ü–æ–¥—Å—á–µ—Ç —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–ø–ª–∏–∫ —É –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
current_counts = antagonists_df["character"].value_counts().to_dict()

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ª–∏–º–∏—Ç—ã, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è, —á—Ç–æ –æ–Ω–∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫
final_limits = {
    "Jon Snow": min(800, current_counts.get("Jon Snow", 0)),
    "SpongeBob": min(1080, current_counts.get("SpongeBob", 0)),
    "Joey Tribbiani": min(1080, current_counts.get("Joey Tribbiani", 0)),
    "Monica Geller": min(1080, current_counts.get("Monica Geller", 0)),
    "Amy Farrah Fowler": min(1080, current_counts.get("Amy Farrah Fowler", 0)),
    "Leonard Hofstadter": min(1080, current_counts.get("Leonard Hofstadter", 0)),
    "Pam Beesly": min(1080, current_counts.get("Pam Beesly", 0)),
    "Penny": min(1080, current_counts.get("Penny", 0)),
    "Tyrion Lannister": min(800, current_counts.get("Tyrion Lannister", 0)),
    "Daenerys Targaryen": min(700, current_counts.get("Daenerys Targaryen", 0)),
    "Stanley Hudson": min(505, current_counts.get("Stanley Hudson", 0))
}

# –í—ã—á–∏—Å–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
target_total = 8557
current_total = sum(final_limits.values())
to_remove = current_total - target_total

# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –±–æ–ª—å—à–µ, —á–µ–º –µ—Å—Ç—å, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ª–∏–º–∏—Ç—ã
if to_remove > 0:
    # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥–µ–º —É–º–µ–Ω—å—à–∞—Ç—å —Ä–µ–ø–ª–∏–∫–∏
    characters_to_reduce = ["SpongeBob", "Joey Tribbiani", "Monica Geller", "Amy Farrah Fowler",
                            "Leonard Hofstadter", "Pam Beesly", "Penny"]

    # –í—ã—á–∏—Å–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫ —É–±—Ä–∞—Ç—å —Å –∫–∞–∂–¥–æ–≥–æ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ)
    to_remove_per_character = to_remove // len(characters_to_reduce)

    # –ï—Å–ª–∏ –æ—Å—Ç–∞–µ—Ç—Å—è "—Ö–≤–æ—Å—Ç" (–æ—Å—Ç–∞—Ç–æ–∫ –æ—Ç –¥–µ–ª–µ–Ω–∏—è), —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –µ–≥–æ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º
    extra_to_remove = to_remove % len(characters_to_reduce)

    # –û–±–Ω–æ–≤–ª—è–µ–º –ª–∏–º–∏—Ç—ã
    for i, character in enumerate(characters_to_reduce):
        new_limit = final_limits[character] - to_remove_per_character - (1 if i < extra_to_remove else 0)
        final_limits[character] = max(0, new_limit)  # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω–µ —É—Ö–æ–¥–∏–º –≤ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–µ–∑–∫—É –ø–æ –Ω–æ–≤—ã–º –ª–∏–º–∏—Ç–∞–º
antagonists_df = antagonists_df.groupby("character").apply(
    lambda x: x.sample(n=min(final_limits.get(x.name, len(x)), len(x)), random_state=42)
).reset_index(drop=True)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫
final_total = len(antagonists_df)
print("üìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫:", final_total)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –ø–æ—Å–ª–µ –æ–±—Ä–µ–∑–∫–∏
final_counts = antagonists_df["character"].value_counts()
print(final_counts)